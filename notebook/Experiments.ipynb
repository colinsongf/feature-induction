{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective feature induction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polysemy often leads to errors in text classifiers. Similarly, polarity shifts can flip the meaning of a sentence while still using the same terms.\n",
    "\n",
    "Here, we investigate ways to \n",
    "- identify terms that are used in ambiguous contexts\n",
    "- introduce new features that resolve that ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our solution is to use \"selective non-linearity\" to refine such features. This works as follows:\n",
    "- Identify terms that are moderately indicative of the positive class.\n",
    "- Collect all instances that contain the term.\n",
    "- Create a decision tree to classify those instances 100% correctly.\n",
    "- Replace the term feature value for all instances as follows:\n",
    "  - if it is 0, it stays 0.\n",
    "  - else, replace the feature value with the output of the classifier.\n",
    "  \n",
    "In this way, we allow non-linear feature value to better discriminate pos/neg instances, but we limit this non-linearity to avoid over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Related Work**\n",
    "\n",
    "- Polarity shifting:\n",
    "  - <http://lexitron.nectec.or.th/public/COLING-2010_Beijing_China/PAPERS/pdf/PAPERS072.pdf>\n",
    "- Subsequence kerlens:\n",
    "  - <http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6890481>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this (real) misclassification error:\n",
    "\n",
    ">truth=1 pred=0 text=it never fails to engage us .  \n",
    ">never fails=0.676812 us=0.486345 it never=0.103713 it=0.0842794 engage=0.0705244|||fails=-0.964354 fails to=-0.539867 never=-0.269861 to=-0.23602 to engage=-0.0702657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import copy\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, _tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_errors(truths, preds, X, raws, idx2clf, feature_names):\n",
    "    for i, (truth, pred, raw) in enumerate(zip(truths, preds, raws)):\n",
    "        if truth != pred:\n",
    "            clf = idx2clf[i]\n",
    "            coef = clf.coef_[0]\n",
    "            print 'truth=%d pred=%d prob=%.5f\\ntext=%s' % (truth, pred, clf.predict_proba(X[i])[0][int(pred)], raw.strip())\n",
    "            dot = np.array(X[i])[0] * coef\n",
    "            print ' '.join(['%s=%g' % (feature_names[i], dot[i]) for i in np.argsort(dot)[::-1][:5] if dot[i] != 0]) + '|||' + \\\n",
    "                   ' '.join(['%s=%g' % (feature_names[i], dot[i]) for i in np.argsort(dot)[:5] if dot[i] != 0])\n",
    "            print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from numpy.linalg import norm\n",
    "import utils\n",
    "# See http://alexminnaar.com/time-series-classification-and-clustering-with-python.html\n",
    "\n",
    "def my_dist(x, y, start=1000):\n",
    "    \"\"\" Allow lists to contain a mix of floats (for coefficients) and ints representing\n",
    "    special keywords (but, and, not). These special ints begin at value start.\"\"\"\n",
    "    if x < start and y < start:  # default L1 or L2 norm\n",
    "        return (x-y)**2  # abs(x-y)\n",
    "    elif x >= start and y >= start:  # both are special keywords.\n",
    "        return 10. if x != y else 0.\n",
    "    else:  # one is a keyword and one is a coefficient.\n",
    "        return 10. \n",
    "    \n",
    "def get_coef(tok, voc, coef):\n",
    "    # polars = {}\n",
    "    polars = {'but': 1000, 'not': 1001}\n",
    "    if tok in polars:\n",
    "        return polars[tok]\n",
    "    elif tok not in voc:\n",
    "        return 0.\n",
    "    else:\n",
    "        return coef[voc[tok]]\n",
    "    \n",
    "def make_time_series(raws, coef, vec):\n",
    "    tker = vec.build_tokenizer()\n",
    "    voc = vec.vocabulary_\n",
    "    serieses = []\n",
    "    for raw in raws:\n",
    "        toks = tker(raw)\n",
    "        series = np.array([get_coef(t, voc, coef) for t in toks])\n",
    "        serieses.append(series)\n",
    "    return serieses\n",
    "\n",
    "class DTWClassifier(object):\n",
    "    \n",
    "    def __init__(self, dist_f, window, neighbors, vec, n_folds, weighted, rand):\n",
    "        if neighbors > 1:\n",
    "            print 'neighbors > 1 not yet implemented'\n",
    "            return\n",
    "        self.n_folds = n_folds\n",
    "        self.dist_f = dist_f\n",
    "        self.vec = vec\n",
    "        self.rand = rand\n",
    "        self.neighbors = neighbors\n",
    "        self.weighted = weighted\n",
    "        self.dtw = utils.DTW(dist=dist_f, window=window)\n",
    "        self.window = window\n",
    "        self.dist_f = dist_f\n",
    "        \n",
    "    def fit(self, X, y, raws):\n",
    "        indices = []\n",
    "        classifiers = []\n",
    "        for train, test in KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=self.rand):\n",
    "            indices.extend(test)\n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(X[train], y[train])\n",
    "            classifiers.append(clf)\n",
    "        # TODO: average classifiers.\n",
    "        avg_coef = np.average([clf.coef_[0] for clf in classifiers], axis=0)\n",
    "        # avg_coef = 1. / (1. + np.exp(avg_coef))  # logistic\n",
    "        self.series = make_time_series(np.array(raws)[indices], avg_coef, self.vec)\n",
    "        self.labels = y[indices]\n",
    "        self.raws = np.array(raws)[indices]\n",
    "        self.logreg = LogisticRegression()\n",
    "        self.logreg.fit(X, y)\n",
    "        self.coef = self.logreg.coef_[0]  # 1. / (1. + np.exp(self.logreg.coef_[0]))  # logistic\n",
    "        print 'logreg training acc=%g' % accuracy_score(self.logreg.predict(X), y)\n",
    "    \n",
    "    def predict(self, X, raws, y):\n",
    "        \n",
    "        test_series = make_time_series(raws, self.coef, self.vec)\n",
    "        predictions = []\n",
    "        for i, (test_s, raw) in enumerate(zip(test_series, raws)):\n",
    "            min_dist=float('inf')\n",
    "            min_j = -1\n",
    "            count = 0\n",
    "            for j, train_s in enumerate(self.series):\n",
    "                if utils.lb_keogh(train_s, test_s, self.dist_f, r=self.window) < min_dist:\n",
    "                    dist = self.dtw.distance(train_s, test_s)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist=dist\n",
    "                        min_j = j\n",
    "                    count += 1\n",
    "            predictions.append(self.labels[min_j])\n",
    "            print '\\ntesting (label=%d)\\n%s' % (y[i], raw.strip())\n",
    "            print 'closest (label=%d)\\n%s' % (self.labels[min_j], self.raws[min_j].strip())\n",
    "            print 'did %d DTW computations' % count\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 1000 instances and 10012 features; label distribution=Counter({1: 502, 0: 498})\n",
      "logreg training acc=0.9975\n",
      "\n",
      "testing (label=0)\n",
      "[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation .\n",
      "closest (label=0)\n",
      "there's no point in extracting the bare bones of byatt's plot for purposes of bland hollywood romance .\n",
      "did 676 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .\n",
      "closest (label=1)\n",
      "i never thought i'd say this , but i'd much rather watch teens poking their genitals into fruit pies !\n",
      "did 291 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "while the performances are often engaging , this loose collection of largely improvised numbers would probably have worked better as a one-hour tv documentary .\n",
      "closest (label=0)\n",
      "stirs potentially enticing ingredients into an uneasy blend of ghost and close encounters of the third kind .\n",
      "did 693 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "on a cutting room floor somewhere lies . . . footage that might have made no such thing a trenchant , ironic cultural satire instead of a frustrating misfire .\n",
      "closest (label=1)\n",
      "kurys never shows why , of all the period's volatile romantic lives , sand and musset are worth particular attention .\n",
      "did 684 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "there is a difference between movies with the courage to go over the top and movies that don't care about being stupid\n",
      "closest (label=0)\n",
      "the dialogue is very choppy and monosyllabic despite the fact that it is being dubbed .\n",
      "did 623 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "such master screenwriting comes courtesy of john pogue , the yale grad who previously gave us \" the skulls \" and last year's \" rollerball . \" enough said , except : film overboard !\n",
      "closest (label=1)\n",
      "never capitalizes on this concept and opts for the breezy and amateurish feel of an after school special on the subject of tolerance .\n",
      "did 617 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "this 100-minute movie only has about 25 minutes of decent material .\n",
      "closest (label=0)\n",
      "the characters are never more than sketches . . . which leaves any true emotional connection or identification frustratingly out of reach .\n",
      "did 650 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "it shows that some studios firmly believe that people have lost the ability to think and will forgive any shoddy product as long as there's a little girl-on-girl action .\n",
      "closest (label=0)\n",
      "looks more like a travel-agency video targeted at people who like to ride bikes topless and roll in the mud than a worthwhile glimpse of independent-community guiding lights .\n",
      "did 653 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "as exciting as all this exoticism might sound to the typical pax viewer , the rest of us will be lulled into a coma .\n",
      "closest (label=1)\n",
      "unfortunately , carvey's rubber-face routine is no match for the insipid script he has crafted with harris goldberg .\n",
      "did 661 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "stupid , infantile , redundant , sloppy , over-the-top , and amateurish . yep , it's \" waking up in reno . \" go back to sleep .\n",
      "closest (label=0)\n",
      "loses its sense of humor in a vat of failed jokes , twitchy acting , and general boorishness .\n",
      "did 682 DTW computations\n",
      "fold 0 acc=0.8\n",
      "logreg training acc=0.99875\n",
      "\n",
      "testing (label=0)\n",
      "exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .\n",
      "closest (label=0)\n",
      "stupid , infantile , redundant , sloppy , over-the-top , and amateurish . yep , it's \" waking up in reno . \" go back to sleep .\n",
      "did 637 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "the story is also as unoriginal as they come , already having been recycled more times than i'd care to count .\n",
      "closest (label=0)\n",
      "every potential twist is telegraphed well in advance , every performance respectably muted ; the movie itself seems to have been made under the influence of rohypnol .\n",
      "did 696 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "about the only thing to give the movie points for is bravado -- to take an entirely stale concept and push it through the audience's meat grinder one more time .\n",
      "closest (label=1)\n",
      "hoffman waits too long to turn his movie in an unexpected direction , and even then his tone retains a genteel , prep-school quality that feels dusty and leatherbound .\n",
      "did 657 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "unfortunately the story and the actors are served with a hack script .\n",
      "closest (label=1)\n",
      "hill looks to be going through the motions , beginning with the pale script .\n",
      "did 754 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "all the more disquieting for its relatively gore-free allusions to the serial murders , but it falls down in its attempts to humanize its subject .\n",
      "closest (label=1)\n",
      "like a pack of dynamite sticks , built for controversy . the film is explosive , but a few of those sticks are wet .\n",
      "did 600 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "the party scenes deliver some tawdry kicks . the rest of the film . . . is dudsville .\n",
      "closest (label=1)\n",
      "just as the lousy tarantino imitations have subsided , here comes the first lousy guy ritchie imitation .\n",
      "did 694 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "somewhere in the middle , the film compels , as demme experiments he harvests a few movie moment gems , but the field of roughage dominates .\n",
      "closest (label=1)\n",
      "this tale has been told and retold ; the races and rackets change , but the song remains the same .\n",
      "did 457 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "i didn't laugh . i didn't smile . i survived .\n",
      "closest (label=0)\n",
      "smothered by its own solemnity .\n",
      "did 734 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "analyze that regurgitates and waters down many of the previous film's successes , with a few new swings thrown in .\n",
      "closest (label=1)\n",
      "a rather average action film that benefits from several funny moments supplied by epps .\n",
      "did 690 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "the country bears has no scenes that will upset or frighten young viewers . unfortunately , there is almost nothing in this flat effort that will amuse or entertain them , either .\n",
      "closest (label=1)\n",
      "the movie attempts to mine laughs from a genre -- the gangster/crime comedy -- that wore out its welcome with audiences several years ago , and its cutesy reliance on movie-specific cliches isn't exactly endearing .\n",
      "did 658 DTW computations\n",
      "fold 1 acc=0.6\n",
      "logreg training acc=0.99625\n",
      "\n",
      "testing (label=1)\n",
      "not so much farcical as sour .\n",
      "closest (label=0)\n",
      "not really a thriller so much as a movie for teens to laugh , groan and hiss at .\n",
      "did 342 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "while the ensemble player who gained notice in guy ritchie's lock , stock and two smoking barrels and snatch has the bod , he's unlikely to become a household name on the basis of his first starring vehicle .\n",
      "closest (label=0)\n",
      "'ejemplo de una cinta en que no importa el talento de su reparto o lo interesante que pudo haber resultado su premisa , pues el resultado es francamente aburrido y , por momentos , deplorable . '\n",
      "did 604 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "the execution is so pedestrian that the most positive comment we can make is that rob schneider actually turns in a pretty convincing performance as a prissy teenage girl .\n",
      "closest (label=0)\n",
      "watching the powerpuff girls movie , my mind kept returning to one anecdote for comparison : the cartoon in japan that gave people seizures .\n",
      "did 420 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "the action clich�s just pile up .\n",
      "closest (label=0)\n",
      "the movie is so thoughtlessly assembled .\n",
      "did 709 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "payami tries to raise some serious issues about iran's electoral process , but the result is a film that's about as subtle as a party political broadcast .\n",
      "closest (label=0)\n",
      "there are things to like about murder by numbers -- but , in the end , the disparate elements don't gel .\n",
      "did 612 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "please , someone , stop eric schaeffer before he makes another film .\n",
      "closest (label=0)\n",
      "the worst film of the year .\n",
      "did 754 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "though the opera itself takes place mostly indoors , jacquot seems unsure of how to evoke any sort of naturalism on the set .\n",
      "closest (label=1)\n",
      "about one in three gags in white's intermittently wise script hits its mark ; the rest are padding unashamedly appropriated from the teen-exploitation playbook .\n",
      "did 653 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "with flashbulb editing as cover for the absence of narrative continuity , undisputed is nearly incoherent , an excuse to get to the closing bout . . . by which time it's impossible to care who wins .\n",
      "closest (label=0)\n",
      "astonishing isn't the word -- neither is incompetent , incoherent or just plain crap . indeed , none of these words really gets at the very special type of badness that is deuces wild .\n",
      "did 648 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "to the civilized mind , a movie like ballistic : ecks vs . sever is more of an ordeal than an amusement .\n",
      "closest (label=0)\n",
      "a wretched movie that reduces the second world war to one man's quest to find an old flame .\n",
      "did 642 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "the lack of naturalness makes everything seem self-consciously poetic and forced . . . it's a pity that [nelson's] achievement doesn't match his ambition .\n",
      "closest (label=0)\n",
      "[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation .\n",
      "did 674 DTW computations\n",
      "fold 2 acc=0.4\n",
      "logreg training acc=1\n",
      "\n",
      "testing (label=1)\n",
      "simplistic , silly and tedious .\n",
      "closest (label=0)\n",
      "banal and predictable .\n",
      "did 627 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "it's so laddish and juvenile , only teenage boys could possibly find it funny .\n",
      "closest (label=0)\n",
      "the filmmakers lack the nerve . . . to fully exploit the script's potential for sick humor .\n",
      "did 757 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "a sentimental mess that never rings true .\n",
      "closest (label=1)\n",
      "the talk-heavy film plays like one of robert altman's lesser works .\n",
      "did 610 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "a farce of a parody of a comedy of a premise , it isn't a comparison to reality so much as it is a commentary about our knowledge of films .\n",
      "closest (label=1)\n",
      "as pure over-the-top trash , any john waters movie has it beat by a country mile .\n",
      "did 672 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "our culture is headed down the toilet with the ferocity of a frozen burrito after an all-night tequila bender � and i know this because i've seen 'jackass : the movie . '\n",
      "closest (label=0)\n",
      "'ejemplo de una cinta en que no importa el talento de su reparto o lo interesante que pudo haber resultado su premisa , pues el resultado es francamente aburrido y , por momentos , deplorable . '\n",
      "did 632 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "the movie's something-borrowed construction feels less the product of loving , well integrated homage and more like a mere excuse for the wan , thinly sketched story . killing time , that's all that's going on here .\n",
      "closest (label=0)\n",
      "while it is interesting to witness the conflict from the palestinian side , longley's film lacks balance . . . and fails to put the struggle into meaningful historical context .\n",
      "did 631 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      ". . . if you're just in the mood for a fun -- but bad -- movie , you might want to catch freaks as a matinee .\n",
      "closest (label=0)\n",
      "the film thrusts the inchoate but already eldritch christian right propaganda machine into national media circles .\n",
      "did 530 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "there's no getting around the fact that this is revenge of the nerds revisited -- again .\n",
      "closest (label=0)\n",
      "one of those films that started with a great premise and then just fell apart .\n",
      "did 722 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "stinks from start to finish , like a wet burlap sack of gloom .\n",
      "closest (label=0)\n",
      "constantly slips from the grasp of its maker .\n",
      "did 620 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "equlibrium could pass for a thirteen-year-old's book report on the totalitarian themes of 1984 and farenheit 451 .\n",
      "closest (label=1)\n",
      "'christian bale's quinn [is] a leather clad grunge-pirate with a hairdo like gandalf in a wind-tunnel and a simply astounding cor-blimey-luv-a-duck cockney accent . '\n",
      "did 627 DTW computations\n",
      "fold 3 acc=0.3\n",
      "logreg training acc=0.9975\n",
      "\n",
      "testing (label=0)\n",
      "interesting , but not compelling .\n",
      "closest (label=0)\n",
      "clever but not especially compelling .\n",
      "did 73 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "nothing here seems as funny as it did in analyze this , not even joe viterelli as de niro's right-hand goombah .\n",
      "closest (label=1)\n",
      "several uninteresting , unlikeable people do bad things to and with each other in \" unfaithful . \" why anyone who is not a character in this movie should care is beyond me .\n",
      "did 533 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "here , common sense flies out the window , along with the hail of bullets , none of which ever seem to hit sascha .\n",
      "closest (label=1)\n",
      "the setting turns out to be more interesting than any of the character dramas , which never reach satisfying conclusions .\n",
      "did 632 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "on its own , it's not very interesting . as a remake , it's a pale imitation .\n",
      "closest (label=0)\n",
      "wow . i have not been this disappointed by a movie in a long time .\n",
      "did 254 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "the criticism never rises above easy , cynical potshots at morally bankrupt characters . . .\n",
      "closest (label=1)\n",
      "really does feel like a short stretched out to feature length .\n",
      "did 677 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "the only surprise is that heavyweights joel silver and robert zemeckis agreed to produce this ; i assume the director has pictures of them cavorting in ladies' underwear .\n",
      "closest (label=1)\n",
      "hugh grant's act is so consuming that sometimes it's difficult to tell who the other actors in the movie are .\n",
      "did 642 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "most of the problems with the film don't derive from the screenplay , but rather the mediocre performances by most of the actors involved\n",
      "closest (label=0)\n",
      "apparently designed as a reverie about memory and regret , but the only thing you'll regret is remembering the experience of sitting through it .\n",
      "did 587 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "the effort is sincere and the results are honest , but the film is so bleak that it's hardly watchable .\n",
      "closest (label=0)\n",
      "adam sandler's heart may be in the right place , but he needs to pull his head out of his butt\n",
      "did 495 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "pc stability notwithstanding , the film suffers from a simplistic narrative and a pat , fairy-tale conclusion .\n",
      "closest (label=1)\n",
      "it's like an all-star salute to disney's cheesy commercialism .\n",
      "did 737 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "an odd , haphazard , and inconsequential romantic comedy .\n",
      "closest (label=0)\n",
      "from beginning to end , this overheated melodrama plays like a student film .\n",
      "did 735 DTW computations\n",
      "fold 4 acc=0.5\n",
      "avg acc=0.52\n"
     ]
    }
   ],
   "source": [
    "def do_polarity_expt_knn():\n",
    "    rand = np.random.RandomState(123456)    \n",
    "    DATA = '/data/polarity/rt-polaritydata'\n",
    "    neg = open(DATA + '/rt-polarity.neg').readlines()\n",
    "    pos = open(DATA + '/rt-polarity.pos').readlines()\n",
    "    y = np.array([0] * len(neg) + [1] * len(pos))\n",
    "    raw = np.array(neg + pos)\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), decode_error='replace', max_df=1.0, min_df=2, binary=True)\n",
    "    X = vectorizer.fit_transform(neg + pos).todense()\n",
    "    # Downsample for testing\n",
    "    sample = rand.choice(np.arange(len(y)), 1000)\n",
    "    X = X[sample]\n",
    "    y = y[sample]    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print 'read %d instances and %d features; label distribution=%s' % (len(y), len(feature_names), Counter(y))\n",
    "    accuracies = []\n",
    "    for fi, (train, test) in enumerate(KFold(len(y), n_folds=5, shuffle=True, random_state=rand)):\n",
    "        test = test[:10]\n",
    "        clf = DTWClassifier(dist_f=my_dist, window=8, neighbors=1, vec=vectorizer, n_folds=10,\n",
    "                            weighted=True, rand=rand)\n",
    "        clf.fit(X[train], y[train], raw[train])\n",
    "        pred = clf.predict(X[test], raw[test], y[test])\n",
    "        accuracies.append(accuracy_score(pred, y[test]))\n",
    "        print 'fold %d acc=%g' % (fi, accuracies[-1])\n",
    "    print 'avg acc=%g' % np.mean(accuracies)\n",
    "\n",
    "reload(utils)\n",
    "do_polarity_expt_knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 10662 instances and 10047 features; label distribution=Counter({0: 5331, 1: 5331})\n",
      "feature_names= [u'000', u'007', u'1', u'10', u'100', u'101', u'102', u'10th', u'11', u'110']\n"
     ]
    }
   ],
   "source": [
    "def do_polarity_expt():\n",
    "    rand = np.random.RandomState(123456)    \n",
    "    DATA = '/data/polarity/rt-polaritydata'\n",
    "    neg = open(DATA + '/rt-polarity.neg').readlines()\n",
    "    pos = open(DATA + '/rt-polarity.pos').readlines()\n",
    "    y = np.array([0] * len(neg) + [1] * len(pos))\n",
    "    raws = np.array(neg + pos)\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), token_pattern=r'(?u)\\b\\w+\\b', decode_error='replace',\n",
    "                                 max_df=1.0, min_df=2, binary=True)\n",
    "    X = vectorizer.fit_transform(neg + pos).todense()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print 'read %d instances and %d features; label distribution=%s' % (len(y), len(feature_names), Counter(y))\n",
    "    print 'feature_names=', feature_names[:10]\n",
    "    indices = []\n",
    "    clfs = []\n",
    "    for train, test in KFold(len(y), n_folds=10, shuffle=True, random_state=rand):\n",
    "        indices.extend(test)\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X[train], y[train])\n",
    "        clfs.append(clf)\n",
    "    avg_coef = np.average([clf.coef_[0] for clf in clfs], axis=0)\n",
    "    series = make_time_series(raws[indices], avg_coef, vectorizer)\n",
    "    return series, raws[indices], y[indices]\n",
    "\n",
    "series, raw, labels = do_polarity_expt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest documents to idx 3203\n",
      "the effort is sincere and the results are honest , but the film is so bleak that it's hardly watchable . \n",
      " -0.01 -0.12 0.15 0.19 0.37 -0.01 -0.65 0.00 0.72 1000.00 -0.01 0.31 0.15 -0.52 0.70 0.12 0.15 0.08 -0.68 0.88\n",
      "label=0\n",
      "\n",
      "idx=7020 dist=1.0424 lbk=0.1635\n",
      "the film is all a little lit crit 101 , but it's extremely well played and often very funny . \n",
      " -0.01 0.31 0.15 -0.25 0.16 -0.52 0.12 0.00 0.53 1000.00 0.15 0.08 -0.52 0.20 -0.32 0.37 0.30 -0.17 1.04\n",
      "label=1\n",
      "\n",
      "idx=2429 dist=1.0597 lbk=0.4803\n",
      "the cast is uniformly excellent . . . but the film itself is merely mildly charming . \n",
      " -0.01 0.20 0.15 -0.45 0.60 1000.00 -0.01 0.31 -0.22 0.15 -0.28 -1.16 1.00\n",
      "label=0\n",
      "\n",
      "idx=1200 dist=1.1679 lbk=0.0000\n",
      "it's a very sincere work , but it would be better as a diary or documentary . \n",
      " 0.15 0.08 0.16 -0.17 0.19 0.34 1000.00 0.15 -0.41 0.05 -0.21 -0.11 0.16 -0.38 -0.40 0.71\n",
      "label=0\n",
      "\n",
      "idx=6992 dist=1.2080 lbk=0.5146\n",
      "if it tried to do anything more , it would fail and perhaps explode , but at this level of manic whimsy , it is just about right . \n",
      " 0.26 0.15 0.01 -0.17 -0.35 -0.23 -0.31 0.15 -0.41 -0.17 0.37 0.39 0.78 1000.00 0.04 -0.12 0.01 0.05 -0.02 0.47 0.15 0.15 -0.24 -0.23 0.48\n",
      "label=1\n",
      "\n",
      "idx=5753 dist=1.2135 lbk=0.0000\n",
      "this isn't a terrible film by any means , but it's also far from being a realized work . \n",
      " -0.12 -0.28 -0.57 0.16 -0.44 0.31 -0.26 0.02 0.37 1000.00 0.15 0.08 0.52 -0.17 0.15 0.11 0.16 -0.66 0.34\n",
      "label=0\n",
      "\n",
      "idx=1554 dist=1.2363 lbk=0.3199\n",
      "bang ! zoom ! it's actually pretty funny , but in all the wrong places . \n",
      " 0.06 0.00 0.15 0.08 -0.10 -0.06 1.04 1000.00 -0.09 -0.25 -0.01 -1.00 0.58\n",
      "label=0\n",
      "\n",
      "idx=2692 dist=1.2523 lbk=0.4605\n",
      "it's traditional moviemaking all the way , but it's done with a lot of careful period attention as well as some very welcome wit . \n",
      " 0.15 0.08 0.09 0.38 -0.25 -0.01 0.62 1000.00 0.15 0.08 0.25 0.34 0.16 0.42 0.05 0.11 -0.49 0.11 -0.11 0.20 -0.11 -0.13 -0.17 0.77 0.23\n",
      "label=1\n",
      "\n",
      "idx=4906 dist=1.2524 lbk=0.2264\n",
      "reign of fire may be little more than another platter of reheated aliens , but it's still pretty tasty . \n",
      " 0.07 0.05 -0.26 0.31 0.05 -0.52 -0.31 -0.18 -0.30 -0.09 0.05 0.00 0.71 1000.00 0.15 0.08 1.23 -0.06 1.10\n",
      "label=1\n",
      "\n",
      "idx=4254 dist=1.2606 lbk=0.2388\n",
      "often demented in a good way , but it is an uneven film for the most part . \n",
      " 0.30 0.14 -0.09 0.16 0.74 0.62 1000.00 0.15 0.15 0.29 -0.88 0.31 -0.18 -0.01 0.06 0.91\n",
      "label=1\n",
      "\n",
      "idx=7049 dist=1.2710 lbk=0.0000\n",
      "we know the plot's a little crazy , but it held my interest from start to finish . \n",
      " 0.10 0.34 -0.01 -0.33 0.08 0.16 -0.52 0.07 1000.00 0.15 0.63 0.35 -0.43 0.15 -0.62 -0.17 0.42\n",
      "label=1\n",
      "\n",
      "idx=6953 dist=1.2919 lbk=0.1017\n",
      "waydowntown is by no means a perfect film , but its boasts a huge charm factor and smacks of originality . \n",
      " 0.15 0.15 -0.26 -0.72 0.37 0.16 0.84 0.31 1000.00 0.24 -0.24 0.16 0.47 0.55 -0.39 0.37 -0.22 0.05 0.95\n",
      "label=1\n",
      "\n",
      "idx=8414 dist=1.3043 lbk=0.0000\n",
      "more vaudeville show than well-constructed narrative , but on those terms it's inoffensive and actually rather sweet . \n",
      " -0.31 0.43 -0.31 -0.18 0.20 0.90 -0.12 1000.00 -0.13 0.28 0.23 0.15 0.08 -0.35 0.37 -0.10 -0.35 0.81\n",
      "label=1\n",
      "\n",
      "idx=7550 dist=1.3088 lbk=0.0000\n",
      "the photographer's show-don't-tell stance is admirable , but it can make him a problematic documentary subject . \n",
      " -0.01 -0.01 0.08 -0.31 0.21 -0.57 -0.16 -0.33 0.15 0.31 1000.00 0.15 -0.04 0.18 -0.22 0.16 -0.54 0.71 -0.00\n",
      "label=0\n",
      "\n",
      "idx=2950 dist=1.3447 lbk=0.4426\n",
      " . . . in no way original , or even all that memorable , but as downtown saturday matinee brain candy , it doesn't disappoint . \n",
      " -0.09 -0.72 0.62 0.03 -0.40 -0.09 -0.25 0.12 0.80 1000.00 -0.11 0.42 0.45 -0.20 0.03 -0.24 0.15 -0.30 -0.57 0.40\n",
      "label=1\n",
      "\n",
      "idx=2345 dist=1.3516 lbk=0.1551\n",
      "birot is a competent enough filmmaker , but her story has nothing fresh or very exciting about it . \n",
      " -0.21 0.15 0.16 0.00 -0.01 0.61 1000.00 0.33 0.11 0.17 -0.80 0.78 -0.40 -0.17 -0.04 -0.23 0.15\n",
      "label=0\n",
      "\n",
      "idx=1671 dist=1.3570 lbk=0.0000\n",
      "a miniscule little bleep on the film radar , but one that many more people should check out\n",
      " 0.16 0.00 -0.52 0.00 -0.13 -0.01 0.31 0.41 1000.00 -0.03 0.12 0.32 -0.31 -0.26 -0.26 0.76 0.07\n",
      "label=1\n",
      "\n",
      "idx=3464 dist=1.3611 lbk=0.0000\n",
      "there's an audience for it , but it could have been funnier and more innocent . \n",
      " -0.26 0.08 0.29 0.34 -0.18 0.15 1000.00 0.15 -0.17 -0.27 -0.10 -0.25 0.37 -0.31 0.33\n",
      "label=0\n",
      "\n",
      "idx=5532 dist=1.3647 lbk=0.0000\n",
      "a cheerful enough but imminently forgettable rip-off of [besson's] earlier work . \n",
      " 0.16 -0.50 -0.01 1000.00 0.00 -0.18 -0.08 -0.52 0.05 -0.04 0.08 -0.30 0.34\n",
      "label=0\n",
      "\n",
      "idx=6736 dist=1.3665 lbk=0.2332\n",
      "thriller directorial debut for traffic scribe gaghan has all the right parts , but the pieces don't quite fit together . \n",
      " 0.07 -0.44 -0.07 -0.18 -0.16 -0.20 -0.28 0.17 -0.25 -0.01 0.48 0.17 1000.00 -0.01 0.10 0.21 -0.57 -0.21 -0.44 0.53\n",
      "label=0\n",
      "\n",
      "idx=7567 dist=1.3689 lbk=0.2540\n",
      "it gets the details of its time frame right but it completely misses its emotions . \n",
      " 0.15 -0.09 -0.01 0.10 0.05 0.24 0.05 -0.16 0.48 1000.00 0.15 -0.18 -0.94 0.24 0.52\n",
      "label=0\n",
      "\n",
      "closest documents to idx 3201\n",
      "interesting , but not compelling . \n",
      " -0.17 1000.00 1001.00 0.87\n",
      "label=0\n",
      "\n",
      "idx=4303 dist=0.4609 lbk=0.1295\n",
      "clever but not especially compelling . \n",
      " -0.04 1000.00 1001.00 0.43 0.87\n",
      "label=0\n",
      "\n",
      "idx=6628 dist=1.4063 lbk=0.0000\n",
      "frenetic but not really funny . \n",
      " -0.27 1000.00 1001.00 -0.52 1.04\n",
      "label=0\n",
      "\n",
      "idx=7368 dist=1.6715 lbk=0.0000\n",
      "overall , interesting as a documentary -- but not very imaxy . \n",
      " -0.43 -0.17 -0.11 0.16 0.71 1000.00 1001.00 -0.17 0.00\n",
      "label=1\n",
      "\n",
      "idx=6504 dist=1.7478 lbk=0.0000\n",
      "it's got the brawn , but not the brains . \n",
      " 0.15 0.08 0.42 -0.01 -0.34 1000.00 1001.00 -0.01 -0.43\n",
      "label=0\n",
      "\n",
      "idx=4990 dist=2.1500 lbk=0.0000\n",
      "a compelling yarn , but not quite a ripping one . \n",
      " 0.16 0.87 0.41 1000.00 1001.00 -0.21 0.16 0.09 -0.03\n",
      "label=1\n",
      "\n",
      "idx=10003 dist=2.2515 lbk=0.0000\n",
      "lillard and cardellini earn their scooby snacks , but not anyone else . \n",
      " 0.28 0.37 0.00 -0.63 0.48 -0.33 0.00 1000.00 1001.00 -0.39 -0.64\n",
      "label=0\n",
      "\n",
      "idx=3573 dist=2.3970 lbk=0.0000\n",
      "sad nonsense , this . but not without cheesy fun factor . \n",
      " 0.18 -0.08 -0.12 1000.00 1001.00 -0.04 -0.91 1.05 -0.39\n",
      "label=0\n",
      "\n",
      "idx=2280 dist=2.8171 lbk=0.0000\n",
      "just an average comedic dateflick but not a waste of time . \n",
      " -0.24 0.29 -0.06 -0.23 0.00 1000.00 1001.00 0.16 -1.55 0.05 0.05\n",
      "label=0\n",
      "\n",
      "idx=3265 dist=2.8819 lbk=0.0000\n",
      "pleasant but not more than recycled jock piffle . \n",
      " 0.85 1000.00 1001.00 -0.31 -0.18 -0.38 0.00 -0.69\n",
      "label=0\n",
      "\n",
      "idx=6220 dist=3.1714 lbk=0.2401\n",
      "witless but watchable . \n",
      " -0.41 1000.00 0.88\n",
      "label=1\n",
      "\n",
      "idx=5292 dist=3.1723 lbk=0.0000\n",
      "formuliac , but fun . \n",
      " 0.00 1000.00 1.05\n",
      "label=1\n",
      "\n",
      "idx=6116 dist=3.1844 lbk=3.1804\n",
      "slight but enjoyable documentary . \n",
      " 0.17 1000.00 1.69 0.71\n",
      "label=1\n",
      "\n",
      "idx=818 dist=3.1959 lbk=3.1670\n",
      "discursive but oddly riveting documentary . \n",
      " 0.00 1000.00 0.51 1.27 0.71\n",
      "label=1\n",
      "\n",
      "idx=10294 dist=3.1972 lbk=0.0000\n",
      "lightweight but appealing . \n",
      " 0.29 1000.00 0.94\n",
      "label=1\n",
      "\n",
      "idx=10519 dist=3.1993 lbk=3.1993\n",
      "sparse but oddly compelling . \n",
      " 0.31 1000.00 0.51 0.87\n",
      "label=1\n",
      "\n",
      "idx=1934 dist=3.2176 lbk=3.1623\n",
      "a modestly made but profoundly moving documentary . \n",
      " 0.16 0.29 -0.18 1000.00 0.28 0.93 0.71\n",
      "label=1\n",
      "\n",
      "idx=7102 dist=3.2224 lbk=3.1623\n",
      "familiar but utterly delightful . \n",
      " 0.31 1000.00 -0.16 1.26\n",
      "label=1\n",
      "\n",
      "idx=3402 dist=3.2315 lbk=0.1735\n",
      "nervous breakdowns are not entertaining . \n",
      " 0.02 0.00 0.00 1001.00 1.48\n",
      "label=0\n",
      "\n",
      "idx=814 dist=3.2462 lbk=3.1803\n",
      "a slight but sweet film . \n",
      " 0.16 0.17 1000.00 0.81 0.31\n",
      "label=1\n",
      "\n",
      "idx=5576 dist=3.2475 lbk=3.1623\n",
      "faultlessly professional but finally slight . \n",
      " 0.00 -0.32 1000.00 -0.31 0.17\n",
      "label=0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out most similar docs.\n",
    "# for doci in [42, 12]:  #[8466, 637, 42, 12]:\n",
    "reload(utils)\n",
    "dtw = utils.DTW(dist=my_dist, window=8)\n",
    "\n",
    "for doci in [3203, 3201]:  # 3203\n",
    "    distances = [dtw.distance(series[doci], s) for s in series]\n",
    "    lbks = [utils.lb_keogh(series[doci], s, dist_f=my_dist, r=8) for s in series]\n",
    "    indices = np.argsort(distances)\n",
    "    print ('closest documents to idx %d\\n%s %s\\nlabel=%d\\n' %\n",
    "           (doci, raw[doci], ' '.join('%.2f' % s for s in series[doci]), labels[doci]))\n",
    "    for i in indices[1:21]:\n",
    "        print('idx=%d dist=%.4f lbk=%.4f\\n%s %s\\nlabel=%d\\n' % (i, distances[i], lbks[i],\n",
    "                                                       raw[i],\n",
    "                                                      ' '.join('%.2f' % s for s in series[i]),\n",
    "                                                      labels[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "\n",
    "```\n",
    "12 interesting , but not compelling . \n",
    "[-0.12858005448526144, -0.13590465658688544, -0.1440977709374012, 0.92921433403940867] \n",
    "label= 0 \n",
    "\n",
    "BAD:   \n",
    "10099 formuliac , but fun . \n",
    "[0, -0.1444148288054018, 1.0599427099457501] \n",
    "label= 1 \n",
    "\n",
    "GOOD:  \n",
    "465 clever but not especially compelling . \n",
    "[0.0079044745193047015, -0.1444148288054018, -0.20212797226837867, 0.72154019776540368, 0.81067640973917521] \n",
    "label= 0 \n",
    "```\n",
    "Order matters...\n",
    "\n",
    "Making \"but\" and \"not\" special tokens resolves this problem.\n",
    "\n",
    "```\n",
    "closest documents to idx=42\n",
    "the effort is sincere and the results are honest , but the film is so bleak that it's hardly watchable . \n",
    " -0.03 0.05 0.23 0.29 0.43 -0.03 -0.76 -0.05 0.93 1000.00 -0.03 0.40 0.23 -0.52 0.84 0.22 0.14 -0.45 1.10\n",
    "label=0\n",
    "\n",
    "GOOD\n",
    "idx=2922 dist=0.1059\n",
    "the cast is uniformly excellent . . . but the film itself is merely mildly charming . \n",
    " -0.03 0.05 0.23 -0.27 0.63 1000.00 -0.03 0.40 -0.49 0.23 -0.03 -1.05 0.82\n",
    "label=0\n",
    "\n",
    "\n",
    "BAD\n",
    "idx=6140 dist=0.1036\n",
    "the film is all a little lit crit 101 , but it's extremely well played and often very funny . \n",
    " -0.06 0.26 0.07 -0.46 -0.38 -0.04 0.00 0.42 1000.00 0.21 -0.50 0.21 -0.46 0.35 0.36 -0.30 1.22\n",
    "label=1\n",
    "```\n",
    "\n",
    "Need to make (.1 - (-.1)) a bigger distance than (.2-.1). We'll try logistic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_newsgroup_expt(categories, ntrees, expt_f):\n",
    "    rand = np.random.RandomState(123456)    \n",
    "    newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                          remove=('headers', 'footers', 'quotes'),\n",
    "                                          categories=categories)\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                         remove=('headers', 'footers', 'quotes'),\n",
    "                                         categories=categories)\n",
    "\n",
    "    vectorizer = CountVectorizer(binary=True, min_df=3)\n",
    "    X_train = vectorizer.fit_transform(newsgroups_train.data).todense()\n",
    "    y_train = newsgroups_train.target\n",
    "    X_test = vectorizer.transform(newsgroups_test.data).todense()\n",
    "    y_test = newsgroups_test.target\n",
    "    print 'train size=', len(y_train), 'test size=', len(y_test)    \n",
    "    expt_f(X_train, y_train, X_test, y_test, vectorizer, ntrees, rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware'], ntrees=100, expt_f=do_expt_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware'], ntrees=100, expt_f=do_expt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['comp.graphics', 'comp.windows.x'], ntrees=100, expt_f=do_expt_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['comp.graphics', 'comp.windows.x'], ntrees=100, expt_f=do_expt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['sci.crypt', 'sci.electronics'], ntrees=100, expt_f=do_expt_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['sci.crypt', 'sci.electronics'], ntrees=100, expt_f=do_expt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMDB\n",
    "# Sample 1k train and 1k test.\n",
    "def top_feats(clf, x, feature_names):\n",
    "    dot = np.array(x[0])[0] * clf.coef_[0]\n",
    "    return ' '.join(['%s=%g' % (feature_names[i], dot[i]) for i in np.argsort(dot)[::-1][:5]]) + '|||' + \\\n",
    "           ' '.join(['%s=%g' % (feature_names[i], dot[i]) for i in np.argsort(dot)[:5]])\n",
    "\n",
    "def do_imdb_expt(ntrees, expt_f):\n",
    "    rand = np.random.RandomState(123456)    \n",
    "    data = pickle.load(open('/data/aclImdb/data.pkl'))\n",
    "    sample = rand.choice(range(len(data[1].train.data)), size=1000, replace=False)\n",
    "    vectorizer = CountVectorizer(binary=True, min_df=4)\n",
    "    X_train = vectorizer.fit_transform(data[1].train.data[sample]).todense()\n",
    "    y_train = data[1].train.target[sample]\n",
    "    X_test = vectorizer.transform(data[1].test.data[sample]).todense()\n",
    "    y_test = data[1].test.target[sample]\n",
    "    print 'train size=', len(y_train), 'test size=', len(y_test)    \n",
    "    feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "    X_test_new = expt_f(X_train, y_train, X_test, y_test, vectorizer, ntrees, rand)    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_og = clf.predict(X_test)\n",
    "    preds_new = clf.predict(X_test_new)\n",
    "    # old right, new wrong:\n",
    "    print 'OLD RIGHT, NEW WRONG:\\n'\n",
    "    for i, (pred_og, pred_new, truth) in enumerate(zip(preds_og, preds_new, y_test)):\n",
    "        text = data[1].test.data[sample][i]\n",
    "        feats = X_test[i]\n",
    "        if pred_og != pred_new and pred_og == truth:\n",
    "            print '\\n\\npred_og=', pred_og, 'pred_new=', pred_new, 'truth=', truth, \\\n",
    "            'top_feats=\\n', top_feats(clf, feats, feature_names), \\\n",
    "            '\\ntop_feats new=\\n', top_feats(clf, X_test_new[i], feature_names), \\\n",
    "            '\\ntop_feats diff=\\n', top_feats(clf, np.abs(X_test_new[i] - feats), feature_names), \\\n",
    "            '\\ninstance=\\n', data[1].test.data[sample][i]\n",
    "\n",
    "    print '\\nOLD WRONG, NEW RIGHT:\\n'\n",
    "    diff_sums = np.array([0] * len(feature_names))\n",
    "    for i, (pred_og, pred_new, truth) in enumerate(zip(preds_og, preds_new, y_test)):\n",
    "        text = data[1].test.data[sample][i]\n",
    "        feats = X_test[i]\n",
    "        if pred_og != pred_new and pred_new == truth:\n",
    "            print '\\n\\npred=', pred_og, 'pred_new=', pred_new, 'truth=', truth, \\\n",
    "            'top_feats=\\n', top_feats(clf, feats, feature_names), \\\n",
    "            '\\ntop_feats new=\\n', top_feats(clf, X_test_new[i], feature_names), \\\n",
    "            '\\ntop_feats diff=\\n', top_feats(clf, np.abs(X_test_new[i] - feats), feature_names), \\\n",
    "            '\\ninstance=\\n', data[1].test.data[sample][i]    \n",
    "            diff_sums += np.abs(X_test_new[i] - feats).tolist()[0]\n",
    "    print 'most changed features:'\n",
    "    print '\\n'.join(['%s %d' % (feature_names[i], diff_sums[i]) for i in np.argsort(diff_sums)[::-1] if diff_sums[i] > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_imdb_expt(ntrees=100, expt_f=do_expt_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_imdb_expt(ntrees=40, expt_f=do_expt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Without retraining classifier:\n",
    "  - Using top 900 features by chi2, test accuracy goes from 0.809524 to 0.845560\n",
    "  - Somewhat non-monotonic increase in accuracy as number of features increases (e.g., accuracy at 1000 is somewhat less; .839125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twokenize(string, lowercase=True, keep_punctuation=False, collapse_urls=True, collapse_mentions=True, collapse_numbers=True):\n",
    "    if not string:\n",
    "        return []\n",
    "    if lowercase:\n",
    "        string = string.lower()\n",
    "    tokens = []\n",
    "    if collapse_urls:\n",
    "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
    "    if collapse_mentions:\n",
    "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
    "    if collapse_numbers:\n",
    "        string = re.sub('[0-9]+', '99', string)\n",
    "    if keep_punctuation:\n",
    "        tokens = string.split()\n",
    "    else:\n",
    "        tokens = re.sub('\\W+', ' ', string).split()\n",
    "    return tokens\n",
    "\n",
    "def do_ecig_expt(ntrees, expt_f):\n",
    "    rand = np.random.RandomState(123456)  \n",
    "    y = []\n",
    "    tweets = []\n",
    "    for row in csv.DictReader(open('/data/2/uic/elaine/AllSamplesPositiveandNeutral.csv')):\n",
    "        y.append(int(row['sent']))\n",
    "        tweets.append(row['text'])\n",
    "    y = np.array(y)\n",
    "    vectorizer = CountVectorizer(decode_error='ignore', ngram_range=(1, 1), max_df=1.0, min_df=2, tokenizer=twokenize, binary=True)\n",
    "    X = vectorizer.fit_transform(tweets).todense()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print 'read %d instances and %d features; label distribution=%s' % (len(y), len(feature_names), Counter(y))\n",
    "    print 'feature_names=', feature_names[:10]\n",
    "    og_preds = []\n",
    "    og_acc = []\n",
    "    new_preds = []\n",
    "    truths = []\n",
    "    new_acc = []\n",
    "    for train, test in KFold(len(y), n_folds=5, shuffle=True, random_state=rand):\n",
    "        truths.extend(y[test])\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X[train], y[train])\n",
    "        og_pred = clf.predict(X[test])\n",
    "        og_preds.extend(og_pred)\n",
    "        og_acc.append(accuracy_score(y[test], og_pred))\n",
    "\n",
    "        X_test, acc = expt_f(X[train], y[train], X[test], y[test], vectorizer, ntrees, rand)\n",
    "        new_acc.append(acc)\n",
    "    print 'og acc=%g' % np.mean(og_acc)\n",
    "    print 'new acc=%g' % np.mean(new_acc)\n",
    "\n",
    "    #y_train = newsgroups_train.target\n",
    "    #X_test = vectorizer.transform(newsgroups_test.data).todense()\n",
    "    #y_test = newsgroups_test.target\n",
    "    #print 'train size=', len(y_train), 'test size=', len(y_test)    \n",
    "    #expt_f(X_train, y_train, X_test, y_test, vectorizer, ntrees, rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_ecig_expt(20, do_expt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_ecig_expt(20, do_expt_cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "python",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
