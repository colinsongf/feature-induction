{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective feature induction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polysemy often leads to errors in text classifiers. Similarly, polarity shifts can flip the meaning of a sentence while still using the same terms.\n",
    "\n",
    "Here, we investigate ways to \n",
    "- identify terms that are used in ambiguous contexts\n",
    "- introduce new features that resolve that ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our solution is to use \"selective non-linearity\" to refine such features. This works as follows:\n",
    "- Identify terms that are moderately indicative of the positive class.\n",
    "- Collect all instances that contain the term.\n",
    "- Create a decision tree to classify those instances 100% correctly.\n",
    "- Replace the term feature value for all instances as follows:\n",
    "  - if it is 0, it stays 0.\n",
    "  - else, replace the feature value with the output of the classifier.\n",
    "  \n",
    "In this way, we allow non-linear feature value to better discriminate pos/neg instances, but we limit this non-linearity to avoid over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Related Work**\n",
    "\n",
    "- Polarity shifting:\n",
    "  - <http://lexitron.nectec.or.th/public/COLING-2010_Beijing_China/PAPERS/pdf/PAPERS072.pdf>\n",
    "- Subsequence kerlens:\n",
    "  - <http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6890481>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this (real) misclassification error:\n",
    "\n",
    ">truth=1 pred=0 text=it never fails to engage us .  \n",
    ">never fails=0.676812 us=0.486345 it never=0.103713 it=0.0842794 engage=0.0705244|||fails=-0.964354 fails to=-0.539867 never=-0.269861 to=-0.23602 to engage=-0.0702657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import copy\n",
    "import csv\n",
    "import heapq\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, _tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_errors(truths, preds, X, raws, idx2clf, feature_names):\n",
    "    for i, (truth, pred, raw) in enumerate(zip(truths, preds, raws)):\n",
    "        if truth != pred:\n",
    "            clf = idx2clf[i]\n",
    "            coef = clf.coef_[0]\n",
    "            print 'truth=%d pred=%d prob=%.5f\\ntext=%s' % (truth, pred, clf.predict_proba(X[i])[0][int(pred)], raw.strip())\n",
    "            dot = np.array(X[i])[0] * coef\n",
    "            print ' '.join(['%s=%g' % (feature_names[i], dot[i]) for i in np.argsort(dot)[::-1][:5] if dot[i] != 0]) + '|||' + \\\n",
    "                   ' '.join(['%s=%g' % (feature_names[i], dot[i]) for i in np.argsort(dot)[:5] if dot[i] != 0])\n",
    "            print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from numpy.linalg import norm\n",
    "import utils\n",
    "# See http://alexminnaar.com/time-series-classification-and-clustering-with-python.html\n",
    "\n",
    "def my_dist(x, y, start=1000):\n",
    "    \"\"\" Allow lists to contain a mix of floats (for coefficients) and ints representing\n",
    "    special keywords (but, and, not). These special ints begin at value start.\"\"\"\n",
    "    if x < start and y < start:  # default L1 or L2 norm\n",
    "        return (x-y)**2  # abs(x-y)\n",
    "    elif x >= start and y >= start:  # both are special keywords.\n",
    "        return 10. if x != y else 0.\n",
    "    else:  # one is a keyword and one is a coefficient.\n",
    "        return 10. \n",
    "    \n",
    "def get_coef(tok, voc, coef):\n",
    "    # polars = {}\n",
    "    polars = {'but': 1000, 'not': 1001}\n",
    "    if tok in polars:\n",
    "        return polars[tok]\n",
    "    elif tok not in voc:\n",
    "        return 0.\n",
    "    else:\n",
    "        return coef[voc[tok]]\n",
    "    \n",
    "def make_time_series(raws, coef, vec):\n",
    "    tker = vec.build_tokenizer()\n",
    "    voc = vec.vocabulary_\n",
    "    serieses = []\n",
    "    for raw in raws:\n",
    "        toks = tker(raw)\n",
    "        series = np.array([get_coef(t, voc, coef) for t in toks])\n",
    "        serieses.append(series)\n",
    "    return serieses\n",
    "\n",
    "class DTWClassifier(object):\n",
    "    \n",
    "    def __init__(self, dist_f, window, neighbors, vec, n_folds, weighted, rand):\n",
    "        self.n_folds = n_folds\n",
    "        self.dist_f = dist_f\n",
    "        self.vec = vec\n",
    "        self.rand = rand\n",
    "        self.neighbors = neighbors\n",
    "        self.weighted = weighted\n",
    "        self.dtw = utils.DTW(dist=dist_f, window=window)\n",
    "        self.window = window\n",
    "        self.dist_f = dist_f\n",
    "        \n",
    "    def fit(self, X, y, raws):\n",
    "        indices = []\n",
    "        classifiers = []\n",
    "        for train, test in KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=self.rand):\n",
    "            indices.extend(test)\n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(X[train], y[train])\n",
    "            classifiers.append(clf)\n",
    "        avg_coef = np.average([clf.coef_[0] for clf in classifiers], axis=0)\n",
    "        # avg_coef = 1. / (1. + np.exp(avg_coef))  # logistic\n",
    "        self.series = make_time_series(np.array(raws)[indices], avg_coef, self.vec)\n",
    "        self.labels = y[indices]\n",
    "        self.raws = np.array(raws)[indices]\n",
    "        self.logreg = LogisticRegression()\n",
    "        self.logreg.fit(X, y)\n",
    "        self.coef = self.logreg.coef_[0]  # 1. / (1. + np.exp(self.logreg.coef_[0]))  # logistic\n",
    "        print 'logreg training acc=%g' % accuracy_score(self.logreg.predict(X), y)\n",
    "    \n",
    "    def predict(self, X, raws, y):\n",
    "        \n",
    "        test_series = make_time_series(raws, self.coef, self.vec)\n",
    "        predictions = []\n",
    "        for i, (test_s, raw) in enumerate(zip(test_series, raws)):\n",
    "            heap = []\n",
    "            for x in range(self.neighbors):\n",
    "                heapq.heappush(heap, (np.inf, -1))\n",
    "            count = 0            \n",
    "            cur_min = np.inf\n",
    "            for j, train_s in enumerate(self.series):\n",
    "                if utils.lb_keogh(train_s, test_s, self.dist_f, r=self.window) < cur_min:\n",
    "                    dist = self.dtw.distance(train_s, test_s)\n",
    "                    if dist < cur_min:\n",
    "                        heapq.heappush(heap, (dist, j, self.labels[j]))\n",
    "                        cur_min = heapq.nsmallest(self.neighbors, heap)[-1][0]\n",
    "                    count += 1\n",
    "            best_matches = heapq.nsmallest(self.neighbors, heap)\n",
    "            pred = int(round(np.mean([v3 for (v1, v2, v3) in best_matches])))\n",
    "            predictions.append(pred)\n",
    "            print '\\ntesting (label=%d)\\n%s' % (y[i], raw.strip())\n",
    "            min_idx = best_matches[0][1]\n",
    "            print 'best_matches=', best_matches\n",
    "            print 'closest (label=%d)\\n%s' % (self.labels[min_idx], self.raws[min_idx].strip())\n",
    "            print 'pred=', pred\n",
    "            print 'did %d DTW computations' % count\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 1000 instances and 10012 features; label distribution=Counter({1: 502, 0: 498})\n",
      "logreg training acc=0.9975\n",
      "\n",
      "testing (label=0)\n",
      "[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation .\n",
      "best_matches= [(0.4131625248193934, 246, 0), (0.4220503645609517, 513, 1), (0.4849760509931247, 348, 0)]\n",
      "closest (label=0)\n",
      "there's no point in extracting the bare bones of byatt's plot for purposes of bland hollywood romance .\n",
      "pred= 0\n",
      "did 713 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .\n",
      "best_matches= [(0.6265676221420007, 540, 1), (0.6271066065018988, 457, 1), (0.6530223437998912, 514, 1)]\n",
      "closest (label=1)\n",
      "i never thought i'd say this , but i'd much rather watch teens poking their genitals into fruit pies !\n",
      "pred= 1\n",
      "did 313 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "while the performances are often engaging , this loose collection of largely improvised numbers would probably have worked better as a one-hour tv documentary .\n",
      "best_matches= [(0.9697386704415114, 769, 0), (0.9954477184016736, 460, 1), (1.0475802930119422, 494, 0)]\n",
      "closest (label=0)\n",
      "stirs potentially enticing ingredients into an uneasy blend of ghost and close encounters of the third kind .\n",
      "pred= 0\n",
      "did 697 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "on a cutting room floor somewhere lies . . . footage that might have made no such thing a trenchant , ironic cultural satire instead of a frustrating misfire .\n",
      "best_matches= [(0.7339986768678506, 539, 1), (0.7502729647709399, 611, 1), (0.7588558999224667, 705, 0)]\n",
      "closest (label=1)\n",
      "kurys never shows why , of all the period's volatile romantic lives , sand and musset are worth particular attention .\n",
      "pred= 1\n",
      "did 688 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "there is a difference between movies with the courage to go over the top and movies that don't care about being stupid\n",
      "best_matches= [(0.6831294023505041, 171, 0), (0.7199599304621414, 591, 1), (0.768343503841348, 238, 1)]\n",
      "closest (label=0)\n",
      "the dialogue is very choppy and monosyllabic despite the fact that it is being dubbed .\n",
      "pred= 1\n",
      "did 675 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "such master screenwriting comes courtesy of john pogue , the yale grad who previously gave us \" the skulls \" and last year's \" rollerball . \" enough said , except : film overboard !\n",
      "best_matches= [(0.632016812643592, 137, 1), (0.6783028839533606, 531, 0), (0.6896917884047192, 27, 1)]\n",
      "closest (label=1)\n",
      "never capitalizes on this concept and opts for the breezy and amateurish feel of an after school special on the subject of tolerance .\n",
      "pred= 1\n",
      "did 639 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "this 100-minute movie only has about 25 minutes of decent material .\n",
      "best_matches= [(0.6265240136739049, 725, 0), (0.6936949326911155, 677, 1), (0.6961154664924242, 452, 0)]\n",
      "closest (label=0)\n",
      "the characters are never more than sketches . . . which leaves any true emotional connection or identification frustratingly out of reach .\n",
      "pred= 0\n",
      "did 684 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "it shows that some studios firmly believe that people have lost the ability to think and will forgive any shoddy product as long as there's a little girl-on-girl action .\n",
      "best_matches= [(0.7908311082820453, 507, 0), (0.8034736714293903, 602, 0), (0.847406947893382, 444, 1)]\n",
      "closest (label=0)\n",
      "looks more like a travel-agency video targeted at people who like to ride bikes topless and roll in the mud than a worthwhile glimpse of independent-community guiding lights .\n",
      "pred= 0\n",
      "did 656 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "as exciting as all this exoticism might sound to the typical pax viewer , the rest of us will be lulled into a coma .\n",
      "best_matches= [(0.6013502613884179, 681, 1), (0.6256953019187745, 653, 0), (0.6288200642149334, 798, 0)]\n",
      "closest (label=1)\n",
      "unfortunately , carvey's rubber-face routine is no match for the insipid script he has crafted with harris goldberg .\n",
      "pred= 0\n",
      "did 668 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "stupid , infantile , redundant , sloppy , over-the-top , and amateurish . yep , it's \" waking up in reno . \" go back to sleep .\n",
      "best_matches= [(0.5370553533902844, 72, 0), (0.5690151749987308, 1, 0), (0.5818204897881321, 281, 1)]\n",
      "closest (label=0)\n",
      "loses its sense of humor in a vat of failed jokes , twitchy acting , and general boorishness .\n",
      "pred= 0\n",
      "did 688 DTW computations\n",
      "fold 0 acc=0.6\n",
      "logreg training acc=0.99875\n",
      "\n",
      "testing (label=0)\n",
      "exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .\n",
      "best_matches= [(0.6022668879343217, 80, 0), (0.6652597147609646, 117, 1), (0.7264298730363535, 67, 1)]\n",
      "closest (label=0)\n",
      "stupid , infantile , redundant , sloppy , over-the-top , and amateurish . yep , it's \" waking up in reno . \" go back to sleep .\n",
      "pred= 1\n",
      "did 700 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "the story is also as unoriginal as they come , already having been recycled more times than i'd care to count .\n",
      "best_matches= [(0.6542328983255993, 172, 0), (0.6804268948296419, 17, 0), (0.6933255801074615, 730, 1)]\n",
      "closest (label=0)\n",
      "every potential twist is telegraphed well in advance , every performance respectably muted ; the movie itself seems to have been made under the influence of rohypnol .\n",
      "pred= 0\n",
      "did 709 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "about the only thing to give the movie points for is bravado -- to take an entirely stale concept and push it through the audience's meat grinder one more time .\n",
      "best_matches= [(1.079371688263685, 318, 1), (1.0901492541680922, 574, 0), (1.2119186279662586, 590, 1)]\n",
      "closest (label=1)\n",
      "hoffman waits too long to turn his movie in an unexpected direction , and even then his tone retains a genteel , prep-school quality that feels dusty and leatherbound .\n",
      "pred= 1\n",
      "did 661 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "unfortunately the story and the actors are served with a hack script .\n",
      "best_matches= [(0.4279235016610628, 457, 1), (0.5240819037368308, 293, 0), (0.5523235099402074, 480, 1)]\n",
      "closest (label=1)\n",
      "hill looks to be going through the motions , beginning with the pale script .\n",
      "pred= 1\n",
      "did 762 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "all the more disquieting for its relatively gore-free allusions to the serial murders , but it falls down in its attempts to humanize its subject .\n",
      "best_matches= [(0.7603830660902923, 77, 1), (0.7776536485475314, 556, 1), (0.8821619028861065, 63, 0)]\n",
      "closest (label=1)\n",
      "like a pack of dynamite sticks , built for controversy . the film is explosive , but a few of those sticks are wet .\n",
      "pred= 1\n",
      "did 617 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "the party scenes deliver some tawdry kicks . the rest of the film . . . is dudsville .\n",
      "best_matches= [(0.47173269702174636, 603, 1), (0.4785624050037123, 575, 0), (0.481809629612125, 711, 1)]\n",
      "closest (label=1)\n",
      "just as the lousy tarantino imitations have subsided , here comes the first lousy guy ritchie imitation .\n",
      "pred= 1\n",
      "did 700 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "somewhere in the middle , the film compels , as demme experiments he harvests a few movie moment gems , but the field of roughage dominates .\n",
      "best_matches= [(0.5273038297388012, 143, 1), (0.6879254091278688, 428, 1), (0.7074880142240162, 511, 1)]\n",
      "closest (label=1)\n",
      "this tale has been told and retold ; the races and rackets change , but the song remains the same .\n",
      "pred= 1\n",
      "did 530 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "i didn't laugh . i didn't smile . i survived .\n",
      "best_matches= [(0.2671532132166128, 794, 0), (0.31504266465923203, 102, 0), (0.34197292793228623, 129, 1)]\n",
      "closest (label=0)\n",
      "smothered by its own solemnity .\n",
      "pred= 0\n",
      "did 759 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "analyze that regurgitates and waters down many of the previous film's successes , with a few new swings thrown in .\n",
      "best_matches= [(0.6747364375889514, 185, 1), (0.6764748594528452, 86, 0), (0.7187237080606128, 231, 1)]\n",
      "closest (label=1)\n",
      "a rather average action film that benefits from several funny moments supplied by epps .\n",
      "pred= 1\n",
      "did 701 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "the country bears has no scenes that will upset or frighten young viewers . unfortunately , there is almost nothing in this flat effort that will amuse or entertain them , either .\n",
      "best_matches= [(1.052669717624048, 194, 1), (1.1494959236160782, 406, 1), (1.183082759913122, 97, 0)]\n",
      "closest (label=1)\n",
      "the movie attempts to mine laughs from a genre -- the gangster/crime comedy -- that wore out its welcome with audiences several years ago , and its cutesy reliance on movie-specific cliches isn't exactly endearing .\n",
      "pred= 1\n",
      "did 661 DTW computations\n",
      "fold 1 acc=0.5\n",
      "logreg training acc=0.99625\n",
      "\n",
      "testing (label=1)\n",
      "not so much farcical as sour .\n",
      "best_matches= [(0.5975443346916349, 773, 0), (0.9933391319549082, 269, 1), (1.1279302212452331, 411, 1)]\n",
      "closest (label=0)\n",
      "not really a thriller so much as a movie for teens to laugh , groan and hiss at .\n",
      "pred= 1\n",
      "did 649 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "while the ensemble player who gained notice in guy ritchie's lock , stock and two smoking barrels and snatch has the bod , he's unlikely to become a household name on the basis of his first starring vehicle .\n",
      "best_matches= [(0.8512616461566483, 412, 0), (0.8614621935234533, 584, 1), (0.8679797099126538, 508, 1)]\n",
      "closest (label=0)\n",
      "'ejemplo de una cinta en que no importa el talento de su reparto o lo interesante que pudo haber resultado su premisa , pues el resultado es francamente aburrido y , por momentos , deplorable . '\n",
      "pred= 1\n",
      "did 619 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "the execution is so pedestrian that the most positive comment we can make is that rob schneider actually turns in a pretty convincing performance as a prissy teenage girl .\n",
      "best_matches= [(0.6142682426822822, 505, 0), (0.6378812399676089, 41, 1), (0.6437696605765613, 344, 1)]\n",
      "closest (label=0)\n",
      "watching the powerpuff girls movie , my mind kept returning to one anecdote for comparison : the cartoon in japan that gave people seizures .\n",
      "pred= 1\n",
      "did 477 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "the action clich�s just pile up .\n",
      "best_matches= [(0.13291355381629882, 236, 0), (0.17079799302636625, 329, 0), (0.19446933567026667, 747, 1)]\n",
      "closest (label=0)\n",
      "the movie is so thoughtlessly assembled .\n",
      "pred= 0\n",
      "did 755 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "payami tries to raise some serious issues about iran's electoral process , but the result is a film that's about as subtle as a party political broadcast .\n",
      "best_matches= [(0.7986763702088218, 546, 0), (0.8253709058921038, 700, 1), (0.8740859514671181, 783, 0)]\n",
      "closest (label=0)\n",
      "there are things to like about murder by numbers -- but , in the end , the disparate elements don't gel .\n",
      "pred= 0\n",
      "did 616 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "please , someone , stop eric schaeffer before he makes another film .\n",
      "best_matches= [(0.4058116439695476, 502, 0), (0.42800968446041543, 238, 0), (0.43095243085532786, 573, 1)]\n",
      "closest (label=0)\n",
      "the worst film of the year .\n",
      "pred= 0\n",
      "did 762 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "though the opera itself takes place mostly indoors , jacquot seems unsure of how to evoke any sort of naturalism on the set .\n",
      "best_matches= [(0.6329242994815932, 74, 1), (0.6370833670838424, 521, 1), (0.6406276014266296, 738, 0)]\n",
      "closest (label=1)\n",
      "about one in three gags in white's intermittently wise script hits its mark ; the rest are padding unashamedly appropriated from the teen-exploitation playbook .\n",
      "pred= 1\n",
      "did 669 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "with flashbulb editing as cover for the absence of narrative continuity , undisputed is nearly incoherent , an excuse to get to the closing bout . . . by which time it's impossible to care who wins .\n",
      "best_matches= [(0.9900241493480316, 267, 0), (1.0351737826943805, 508, 1), (1.0443682353072414, 57, 1)]\n",
      "closest (label=0)\n",
      "astonishing isn't the word -- neither is incompetent , incoherent or just plain crap . indeed , none of these words really gets at the very special type of badness that is deuces wild .\n",
      "pred= 1\n",
      "did 649 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "to the civilized mind , a movie like ballistic : ecks vs . sever is more of an ordeal than an amusement .\n",
      "best_matches= [(0.4565604069253573, 448, 0), (0.6000007649367318, 605, 0), (0.627326802481519, 110, 1)]\n",
      "closest (label=0)\n",
      "a wretched movie that reduces the second world war to one man's quest to find an old flame .\n",
      "pred= 0\n",
      "did 687 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "the lack of naturalness makes everything seem self-consciously poetic and forced . . . it's a pity that [nelson's] achievement doesn't match his ambition .\n",
      "best_matches= [(0.6679491806112632, 480, 0), (0.6682685170313737, 298, 0), (0.671360071369337, 755, 1)]\n",
      "closest (label=0)\n",
      "[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation .\n",
      "pred= 0\n",
      "did 679 DTW computations\n",
      "fold 2 acc=0.4\n",
      "logreg training acc=1\n",
      "\n",
      "testing (label=1)\n",
      "simplistic , silly and tedious .\n",
      "best_matches= [(0.24955502713158229, 114, 0), (0.46298730390361365, 318, 1), (0.47963147206761103, 316, 1)]\n",
      "closest (label=0)\n",
      "banal and predictable .\n",
      "pred= 1\n",
      "did 787 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "it's so laddish and juvenile , only teenage boys could possibly find it funny .\n",
      "best_matches= [(0.5870552631603365, 622, 0), (0.6048644584086005, 520, 0), (0.6408169486603581, 411, 0)]\n",
      "closest (label=0)\n",
      "the filmmakers lack the nerve . . . to fully exploit the script's potential for sick humor .\n",
      "pred= 0\n",
      "did 762 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "a sentimental mess that never rings true .\n",
      "best_matches= [(0.3547960479352903, 603, 1), (0.4454128073075311, 338, 0), (0.49095981508043635, 18, 1)]\n",
      "closest (label=1)\n",
      "the talk-heavy film plays like one of robert altman's lesser works .\n",
      "pred= 1\n",
      "did 750 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "a farce of a parody of a comedy of a premise , it isn't a comparison to reality so much as it is a commentary about our knowledge of films .\n",
      "best_matches= [(0.6514760889954433, 105, 1), (0.7068054680153881, 640, 0), (0.7836894263320973, 117, 1)]\n",
      "closest (label=1)\n",
      "as pure over-the-top trash , any john waters movie has it beat by a country mile .\n",
      "pred= 1\n",
      "did 686 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "our culture is headed down the toilet with the ferocity of a frozen burrito after an all-night tequila bender � and i know this because i've seen 'jackass : the movie . '\n",
      "best_matches= [(0.708477953326628, 249, 0), (0.7410503949032933, 124, 0), (0.8052181994832881, 372, 1)]\n",
      "closest (label=0)\n",
      "'ejemplo de una cinta en que no importa el talento de su reparto o lo interesante que pudo haber resultado su premisa , pues el resultado es francamente aburrido y , por momentos , deplorable . '\n",
      "pred= 0\n",
      "did 647 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "the movie's something-borrowed construction feels less the product of loving , well integrated homage and more like a mere excuse for the wan , thinly sketched story . killing time , that's all that's going on here .\n",
      "best_matches= [(0.9068925607389656, 577, 0), (0.9139235239875401, 555, 0), (0.937692200682668, 315, 0)]\n",
      "closest (label=0)\n",
      "while it is interesting to witness the conflict from the palestinian side , longley's film lacks balance . . . and fails to put the struggle into meaningful historical context .\n",
      "pred= 0\n",
      "did 635 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      ". . . if you're just in the mood for a fun -- but bad -- movie , you might want to catch freaks as a matinee .\n",
      "best_matches= [(1.2093569960474644, 710, 0), (1.26559485900128, 293, 1), (1.3170063760350874, 70, 0)]\n",
      "closest (label=0)\n",
      "the film thrusts the inchoate but already eldritch christian right propaganda machine into national media circles .\n",
      "pred= 0\n",
      "did 533 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "there's no getting around the fact that this is revenge of the nerds revisited -- again .\n",
      "best_matches= [(0.49384916216286084, 446, 0), (0.4945750831529914, 64, 0), (0.4975860504784937, 591, 0)]\n",
      "closest (label=0)\n",
      "one of those films that started with a great premise and then just fell apart .\n",
      "pred= 0\n",
      "did 728 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "stinks from start to finish , like a wet burlap sack of gloom .\n",
      "best_matches= [(0.1664399913914681, 394, 0), (0.21018414204915145, 541, 1), (0.23737448481779008, 129, 0)]\n",
      "closest (label=0)\n",
      "constantly slips from the grasp of its maker .\n",
      "pred= 0\n",
      "did 681 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "equlibrium could pass for a thirteen-year-old's book report on the totalitarian themes of 1984 and farenheit 451 .\n",
      "best_matches= [(0.3527188833379378, 237, 1), (0.41834556584293847, 154, 0), (0.4475264612974007, 345, 0)]\n",
      "closest (label=1)\n",
      "'christian bale's quinn [is] a leather clad grunge-pirate with a hairdo like gandalf in a wind-tunnel and a simply astounding cor-blimey-luv-a-duck cockney accent . '\n",
      "pred= 0\n",
      "did 661 DTW computations\n",
      "fold 3 acc=0.5\n",
      "logreg training acc=0.9975\n",
      "\n",
      "testing (label=0)\n",
      "interesting , but not compelling .\n",
      "best_matches= [(0.5348815606391258, 519, 0), (0.9443123681300256, 152, 0), (1.4714092233434244, 215, 0)]\n",
      "closest (label=0)\n",
      "clever but not especially compelling .\n",
      "pred= 0\n",
      "did 94 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "nothing here seems as funny as it did in analyze this , not even joe viterelli as de niro's right-hand goombah .\n",
      "best_matches= [(1.0320211308843255, 222, 1), (1.0358767897669257, 752, 0), (1.0652627402138064, 593, 0)]\n",
      "closest (label=1)\n",
      "several uninteresting , unlikeable people do bad things to and with each other in \" unfaithful . \" why anyone who is not a character in this movie should care is beyond me .\n",
      "pred= 0\n",
      "did 580 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "here , common sense flies out the window , along with the hail of bullets , none of which ever seem to hit sascha .\n",
      "best_matches= [(0.47053928462131545, 504, 1), (0.5280599839777528, 252, 1), (0.5340964144246547, 81, 0)]\n",
      "closest (label=1)\n",
      "the setting turns out to be more interesting than any of the character dramas , which never reach satisfying conclusions .\n",
      "pred= 1\n",
      "did 645 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "on its own , it's not very interesting . as a remake , it's a pale imitation .\n",
      "best_matches= [(0.6604139358237722, 10, 0), (0.7482544772359762, 472, 1), (0.8495400449320498, 443, 1)]\n",
      "closest (label=0)\n",
      "wow . i have not been this disappointed by a movie in a long time .\n",
      "pred= 1\n",
      "did 266 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "the criticism never rises above easy , cynical potshots at morally bankrupt characters . . .\n",
      "best_matches= [(0.2628627463568204, 346, 1), (0.28762498143114273, 657, 0), (0.3141774912876941, 685, 0)]\n",
      "closest (label=1)\n",
      "really does feel like a short stretched out to feature length .\n",
      "pred= 0\n",
      "did 692 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "the only surprise is that heavyweights joel silver and robert zemeckis agreed to produce this ; i assume the director has pictures of them cavorting in ladies' underwear .\n",
      "best_matches= [(0.6686601088754326, 599, 1), (0.6894947239230161, 53, 1), (0.7101867207854613, 268, 0)]\n",
      "closest (label=1)\n",
      "hugh grant's act is so consuming that sometimes it's difficult to tell who the other actors in the movie are .\n",
      "pred= 1\n",
      "did 651 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "most of the problems with the film don't derive from the screenplay , but rather the mediocre performances by most of the actors involved\n",
      "best_matches= [(1.102239336485675, 444, 0), (1.2216377728216694, 364, 0), (1.2222999897692652, 235, 0)]\n",
      "closest (label=0)\n",
      "apparently designed as a reverie about memory and regret , but the only thing you'll regret is remembering the experience of sitting through it .\n",
      "pred= 0\n",
      "did 600 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "the effort is sincere and the results are honest , but the film is so bleak that it's hardly watchable .\n",
      "best_matches= [(0.8576295350564119, 8, 0), (0.9633741192709547, 784, 1), (1.00428821756199, 394, 1)]\n",
      "closest (label=0)\n",
      "adam sandler's heart may be in the right place , but he needs to pull his head out of his butt\n",
      "pred= 1\n",
      "did 513 DTW computations\n",
      "\n",
      "testing (label=1)\n",
      "pc stability notwithstanding , the film suffers from a simplistic narrative and a pat , fairy-tale conclusion .\n",
      "best_matches= [(0.5176968906362974, 180, 1), (0.5433369124726394, 56, 1), (0.6360848285867092, 65, 0)]\n",
      "closest (label=1)\n",
      "it's like an all-star salute to disney's cheesy commercialism .\n",
      "pred= 1\n",
      "did 761 DTW computations\n",
      "\n",
      "testing (label=0)\n",
      "an odd , haphazard , and inconsequential romantic comedy .\n",
      "best_matches= [(0.44503425791681045, 697, 0), (0.4893384802620786, 516, 0), (0.5089239326421102, 682, 0)]\n",
      "closest (label=0)\n",
      "from beginning to end , this overheated melodrama plays like a student film .\n",
      "pred= 0\n",
      "did 758 DTW computations\n",
      "fold 4 acc=0.5\n",
      "avg acc=0.5\n"
     ]
    }
   ],
   "source": [
    "def do_polarity_expt_knn():\n",
    "    rand = np.random.RandomState(123456)    \n",
    "    DATA = '/data/polarity/rt-polaritydata'\n",
    "    neg = open(DATA + '/rt-polarity.neg').readlines()\n",
    "    pos = open(DATA + '/rt-polarity.pos').readlines()\n",
    "    y = np.array([0] * len(neg) + [1] * len(pos))\n",
    "    raw = np.array(neg + pos)\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), decode_error='replace', max_df=1.0, min_df=2, binary=True)\n",
    "    X = vectorizer.fit_transform(neg + pos).todense()\n",
    "    # Downsample for testing\n",
    "    sample = rand.choice(np.arange(len(y)), 1000)\n",
    "    X = X[sample]\n",
    "    y = y[sample]    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print 'read %d instances and %d features; label distribution=%s' % (len(y), len(feature_names), Counter(y))\n",
    "    accuracies = []\n",
    "    for fi, (train, test) in enumerate(KFold(len(y), n_folds=5, shuffle=True, random_state=rand)):\n",
    "        test = test[:10]\n",
    "        clf = DTWClassifier(dist_f=my_dist, window=8, neighbors=3, vec=vectorizer, n_folds=10,\n",
    "                            weighted=True, rand=rand)\n",
    "        clf.fit(X[train], y[train], raw[train])\n",
    "        pred = clf.predict(X[test], raw[test], y[test])\n",
    "        accuracies.append(accuracy_score(pred, y[test]))\n",
    "        print 'fold %d acc=%g' % (fi, accuracies[-1])\n",
    "    print 'avg acc=%g' % np.mean(accuracies)\n",
    "\n",
    "reload(utils)\n",
    "do_polarity_expt_knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 10662 instances and 10047 features; label distribution=Counter({0: 5331, 1: 5331})\n",
      "feature_names= [u'000', u'007', u'1', u'10', u'100', u'101', u'102', u'10th', u'11', u'110']\n"
     ]
    }
   ],
   "source": [
    "def do_polarity_expt():\n",
    "    rand = np.random.RandomState(123456)    \n",
    "    DATA = '/data/polarity/rt-polaritydata'\n",
    "    neg = open(DATA + '/rt-polarity.neg').readlines()\n",
    "    pos = open(DATA + '/rt-polarity.pos').readlines()\n",
    "    y = np.array([0] * len(neg) + [1] * len(pos))\n",
    "    raws = np.array(neg + pos)\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), token_pattern=r'(?u)\\b\\w+\\b', decode_error='replace',\n",
    "                                 max_df=1.0, min_df=2, binary=True)\n",
    "    X = vectorizer.fit_transform(neg + pos).todense()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print 'read %d instances and %d features; label distribution=%s' % (len(y), len(feature_names), Counter(y))\n",
    "    print 'feature_names=', feature_names[:10]\n",
    "    indices = []\n",
    "    clfs = []\n",
    "    for train, test in KFold(len(y), n_folds=10, shuffle=True, random_state=rand):\n",
    "        indices.extend(test)\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X[train], y[train])\n",
    "        clfs.append(clf)\n",
    "    avg_coef = np.average([clf.coef_[0] for clf in clfs], axis=0)\n",
    "    series = make_time_series(raws[indices], avg_coef, vectorizer)\n",
    "    return series, raws[indices], y[indices]\n",
    "\n",
    "series, raw, labels = do_polarity_expt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest documents to idx 3203\n",
      "the effort is sincere and the results are honest , but the film is so bleak that it's hardly watchable . \n",
      " -0.01 -0.12 0.15 0.19 0.37 -0.01 -0.65 0.00 0.72 1000.00 -0.01 0.31 0.15 -0.52 0.70 0.12 0.15 0.08 -0.68 0.88\n",
      "label=0\n",
      "\n",
      "idx=7020 dist=1.0424 lbk=0.1635\n",
      "the film is all a little lit crit 101 , but it's extremely well played and often very funny . \n",
      " -0.01 0.31 0.15 -0.25 0.16 -0.52 0.12 0.00 0.53 1000.00 0.15 0.08 -0.52 0.20 -0.32 0.37 0.30 -0.17 1.04\n",
      "label=1\n",
      "\n",
      "idx=2429 dist=1.0597 lbk=0.4803\n",
      "the cast is uniformly excellent . . . but the film itself is merely mildly charming . \n",
      " -0.01 0.20 0.15 -0.45 0.60 1000.00 -0.01 0.31 -0.22 0.15 -0.28 -1.16 1.00\n",
      "label=0\n",
      "\n",
      "idx=1200 dist=1.1679 lbk=0.0000\n",
      "it's a very sincere work , but it would be better as a diary or documentary . \n",
      " 0.15 0.08 0.16 -0.17 0.19 0.34 1000.00 0.15 -0.41 0.05 -0.21 -0.11 0.16 -0.38 -0.40 0.71\n",
      "label=0\n",
      "\n",
      "idx=6992 dist=1.2080 lbk=0.5146\n",
      "if it tried to do anything more , it would fail and perhaps explode , but at this level of manic whimsy , it is just about right . \n",
      " 0.26 0.15 0.01 -0.17 -0.35 -0.23 -0.31 0.15 -0.41 -0.17 0.37 0.39 0.78 1000.00 0.04 -0.12 0.01 0.05 -0.02 0.47 0.15 0.15 -0.24 -0.23 0.48\n",
      "label=1\n",
      "\n",
      "idx=5753 dist=1.2135 lbk=0.0000\n",
      "this isn't a terrible film by any means , but it's also far from being a realized work . \n",
      " -0.12 -0.28 -0.57 0.16 -0.44 0.31 -0.26 0.02 0.37 1000.00 0.15 0.08 0.52 -0.17 0.15 0.11 0.16 -0.66 0.34\n",
      "label=0\n",
      "\n",
      "idx=1554 dist=1.2363 lbk=0.3199\n",
      "bang ! zoom ! it's actually pretty funny , but in all the wrong places . \n",
      " 0.06 0.00 0.15 0.08 -0.10 -0.06 1.04 1000.00 -0.09 -0.25 -0.01 -1.00 0.58\n",
      "label=0\n",
      "\n",
      "idx=2692 dist=1.2523 lbk=0.4605\n",
      "it's traditional moviemaking all the way , but it's done with a lot of careful period attention as well as some very welcome wit . \n",
      " 0.15 0.08 0.09 0.38 -0.25 -0.01 0.62 1000.00 0.15 0.08 0.25 0.34 0.16 0.42 0.05 0.11 -0.49 0.11 -0.11 0.20 -0.11 -0.13 -0.17 0.77 0.23\n",
      "label=1\n",
      "\n",
      "idx=4906 dist=1.2524 lbk=0.2264\n",
      "reign of fire may be little more than another platter of reheated aliens , but it's still pretty tasty . \n",
      " 0.07 0.05 -0.26 0.31 0.05 -0.52 -0.31 -0.18 -0.30 -0.09 0.05 0.00 0.71 1000.00 0.15 0.08 1.23 -0.06 1.10\n",
      "label=1\n",
      "\n",
      "idx=4254 dist=1.2606 lbk=0.2388\n",
      "often demented in a good way , but it is an uneven film for the most part . \n",
      " 0.30 0.14 -0.09 0.16 0.74 0.62 1000.00 0.15 0.15 0.29 -0.88 0.31 -0.18 -0.01 0.06 0.91\n",
      "label=1\n",
      "\n",
      "idx=7049 dist=1.2710 lbk=0.0000\n",
      "we know the plot's a little crazy , but it held my interest from start to finish . \n",
      " 0.10 0.34 -0.01 -0.33 0.08 0.16 -0.52 0.07 1000.00 0.15 0.63 0.35 -0.43 0.15 -0.62 -0.17 0.42\n",
      "label=1\n",
      "\n",
      "idx=6953 dist=1.2919 lbk=0.1017\n",
      "waydowntown is by no means a perfect film , but its boasts a huge charm factor and smacks of originality . \n",
      " 0.15 0.15 -0.26 -0.72 0.37 0.16 0.84 0.31 1000.00 0.24 -0.24 0.16 0.47 0.55 -0.39 0.37 -0.22 0.05 0.95\n",
      "label=1\n",
      "\n",
      "idx=8414 dist=1.3043 lbk=0.0000\n",
      "more vaudeville show than well-constructed narrative , but on those terms it's inoffensive and actually rather sweet . \n",
      " -0.31 0.43 -0.31 -0.18 0.20 0.90 -0.12 1000.00 -0.13 0.28 0.23 0.15 0.08 -0.35 0.37 -0.10 -0.35 0.81\n",
      "label=1\n",
      "\n",
      "idx=7550 dist=1.3088 lbk=0.0000\n",
      "the photographer's show-don't-tell stance is admirable , but it can make him a problematic documentary subject . \n",
      " -0.01 -0.01 0.08 -0.31 0.21 -0.57 -0.16 -0.33 0.15 0.31 1000.00 0.15 -0.04 0.18 -0.22 0.16 -0.54 0.71 -0.00\n",
      "label=0\n",
      "\n",
      "idx=2950 dist=1.3447 lbk=0.4426\n",
      " . . . in no way original , or even all that memorable , but as downtown saturday matinee brain candy , it doesn't disappoint . \n",
      " -0.09 -0.72 0.62 0.03 -0.40 -0.09 -0.25 0.12 0.80 1000.00 -0.11 0.42 0.45 -0.20 0.03 -0.24 0.15 -0.30 -0.57 0.40\n",
      "label=1\n",
      "\n",
      "idx=2345 dist=1.3516 lbk=0.1551\n",
      "birot is a competent enough filmmaker , but her story has nothing fresh or very exciting about it . \n",
      " -0.21 0.15 0.16 0.00 -0.01 0.61 1000.00 0.33 0.11 0.17 -0.80 0.78 -0.40 -0.17 -0.04 -0.23 0.15\n",
      "label=0\n",
      "\n",
      "idx=1671 dist=1.3570 lbk=0.0000\n",
      "a miniscule little bleep on the film radar , but one that many more people should check out\n",
      " 0.16 0.00 -0.52 0.00 -0.13 -0.01 0.31 0.41 1000.00 -0.03 0.12 0.32 -0.31 -0.26 -0.26 0.76 0.07\n",
      "label=1\n",
      "\n",
      "idx=3464 dist=1.3611 lbk=0.0000\n",
      "there's an audience for it , but it could have been funnier and more innocent . \n",
      " -0.26 0.08 0.29 0.34 -0.18 0.15 1000.00 0.15 -0.17 -0.27 -0.10 -0.25 0.37 -0.31 0.33\n",
      "label=0\n",
      "\n",
      "idx=5532 dist=1.3647 lbk=0.0000\n",
      "a cheerful enough but imminently forgettable rip-off of [besson's] earlier work . \n",
      " 0.16 -0.50 -0.01 1000.00 0.00 -0.18 -0.08 -0.52 0.05 -0.04 0.08 -0.30 0.34\n",
      "label=0\n",
      "\n",
      "idx=6736 dist=1.3665 lbk=0.2332\n",
      "thriller directorial debut for traffic scribe gaghan has all the right parts , but the pieces don't quite fit together . \n",
      " 0.07 -0.44 -0.07 -0.18 -0.16 -0.20 -0.28 0.17 -0.25 -0.01 0.48 0.17 1000.00 -0.01 0.10 0.21 -0.57 -0.21 -0.44 0.53\n",
      "label=0\n",
      "\n",
      "idx=7567 dist=1.3689 lbk=0.2540\n",
      "it gets the details of its time frame right but it completely misses its emotions . \n",
      " 0.15 -0.09 -0.01 0.10 0.05 0.24 0.05 -0.16 0.48 1000.00 0.15 -0.18 -0.94 0.24 0.52\n",
      "label=0\n",
      "\n",
      "closest documents to idx 3201\n",
      "interesting , but not compelling . \n",
      " -0.17 1000.00 1001.00 0.87\n",
      "label=0\n",
      "\n",
      "idx=4303 dist=0.4609 lbk=0.1295\n",
      "clever but not especially compelling . \n",
      " -0.04 1000.00 1001.00 0.43 0.87\n",
      "label=0\n",
      "\n",
      "idx=6628 dist=1.4063 lbk=0.0000\n",
      "frenetic but not really funny . \n",
      " -0.27 1000.00 1001.00 -0.52 1.04\n",
      "label=0\n",
      "\n",
      "idx=7368 dist=1.6715 lbk=0.0000\n",
      "overall , interesting as a documentary -- but not very imaxy . \n",
      " -0.43 -0.17 -0.11 0.16 0.71 1000.00 1001.00 -0.17 0.00\n",
      "label=1\n",
      "\n",
      "idx=6504 dist=1.7478 lbk=0.0000\n",
      "it's got the brawn , but not the brains . \n",
      " 0.15 0.08 0.42 -0.01 -0.34 1000.00 1001.00 -0.01 -0.43\n",
      "label=0\n",
      "\n",
      "idx=4990 dist=2.1500 lbk=0.0000\n",
      "a compelling yarn , but not quite a ripping one . \n",
      " 0.16 0.87 0.41 1000.00 1001.00 -0.21 0.16 0.09 -0.03\n",
      "label=1\n",
      "\n",
      "idx=10003 dist=2.2515 lbk=0.0000\n",
      "lillard and cardellini earn their scooby snacks , but not anyone else . \n",
      " 0.28 0.37 0.00 -0.63 0.48 -0.33 0.00 1000.00 1001.00 -0.39 -0.64\n",
      "label=0\n",
      "\n",
      "idx=3573 dist=2.3970 lbk=0.0000\n",
      "sad nonsense , this . but not without cheesy fun factor . \n",
      " 0.18 -0.08 -0.12 1000.00 1001.00 -0.04 -0.91 1.05 -0.39\n",
      "label=0\n",
      "\n",
      "idx=2280 dist=2.8171 lbk=0.0000\n",
      "just an average comedic dateflick but not a waste of time . \n",
      " -0.24 0.29 -0.06 -0.23 0.00 1000.00 1001.00 0.16 -1.55 0.05 0.05\n",
      "label=0\n",
      "\n",
      "idx=3265 dist=2.8819 lbk=0.0000\n",
      "pleasant but not more than recycled jock piffle . \n",
      " 0.85 1000.00 1001.00 -0.31 -0.18 -0.38 0.00 -0.69\n",
      "label=0\n",
      "\n",
      "idx=6220 dist=3.1714 lbk=0.2401\n",
      "witless but watchable . \n",
      " -0.41 1000.00 0.88\n",
      "label=1\n",
      "\n",
      "idx=5292 dist=3.1723 lbk=0.0000\n",
      "formuliac , but fun . \n",
      " 0.00 1000.00 1.05\n",
      "label=1\n",
      "\n",
      "idx=6116 dist=3.1844 lbk=3.1804\n",
      "slight but enjoyable documentary . \n",
      " 0.17 1000.00 1.69 0.71\n",
      "label=1\n",
      "\n",
      "idx=818 dist=3.1959 lbk=3.1670\n",
      "discursive but oddly riveting documentary . \n",
      " 0.00 1000.00 0.51 1.27 0.71\n",
      "label=1\n",
      "\n",
      "idx=10294 dist=3.1972 lbk=0.0000\n",
      "lightweight but appealing . \n",
      " 0.29 1000.00 0.94\n",
      "label=1\n",
      "\n",
      "idx=10519 dist=3.1993 lbk=3.1993\n",
      "sparse but oddly compelling . \n",
      " 0.31 1000.00 0.51 0.87\n",
      "label=1\n",
      "\n",
      "idx=1934 dist=3.2176 lbk=3.1623\n",
      "a modestly made but profoundly moving documentary . \n",
      " 0.16 0.29 -0.18 1000.00 0.28 0.93 0.71\n",
      "label=1\n",
      "\n",
      "idx=7102 dist=3.2224 lbk=3.1623\n",
      "familiar but utterly delightful . \n",
      " 0.31 1000.00 -0.16 1.26\n",
      "label=1\n",
      "\n",
      "idx=3402 dist=3.2315 lbk=0.1735\n",
      "nervous breakdowns are not entertaining . \n",
      " 0.02 0.00 0.00 1001.00 1.48\n",
      "label=0\n",
      "\n",
      "idx=814 dist=3.2462 lbk=3.1803\n",
      "a slight but sweet film . \n",
      " 0.16 0.17 1000.00 0.81 0.31\n",
      "label=1\n",
      "\n",
      "idx=5576 dist=3.2475 lbk=3.1623\n",
      "faultlessly professional but finally slight . \n",
      " 0.00 -0.32 1000.00 -0.31 0.17\n",
      "label=0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out most similar docs.\n",
    "# for doci in [42, 12]:  #[8466, 637, 42, 12]:\n",
    "reload(utils)\n",
    "dtw = utils.DTW(dist=my_dist, window=8)\n",
    "\n",
    "for doci in [3203, 3201]:  # 3203\n",
    "    distances = [dtw.distance(series[doci], s) for s in series]\n",
    "    lbks = [utils.lb_keogh(series[doci], s, dist_f=my_dist, r=8) for s in series]\n",
    "    indices = np.argsort(distances)\n",
    "    print ('closest documents to idx %d\\n%s %s\\nlabel=%d\\n' %\n",
    "           (doci, raw[doci], ' '.join('%.2f' % s for s in series[doci]), labels[doci]))\n",
    "    for i in indices[1:21]:\n",
    "        print('idx=%d dist=%.4f lbk=%.4f\\n%s %s\\nlabel=%d\\n' % (i, distances[i], lbks[i],\n",
    "                                                       raw[i],\n",
    "                                                      ' '.join('%.2f' % s for s in series[i]),\n",
    "                                                      labels[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "\n",
    "```\n",
    "12 interesting , but not compelling . \n",
    "[-0.12858005448526144, -0.13590465658688544, -0.1440977709374012, 0.92921433403940867] \n",
    "label= 0 \n",
    "\n",
    "BAD:   \n",
    "10099 formuliac , but fun . \n",
    "[0, -0.1444148288054018, 1.0599427099457501] \n",
    "label= 1 \n",
    "\n",
    "GOOD:  \n",
    "465 clever but not especially compelling . \n",
    "[0.0079044745193047015, -0.1444148288054018, -0.20212797226837867, 0.72154019776540368, 0.81067640973917521] \n",
    "label= 0 \n",
    "```\n",
    "Order matters...\n",
    "\n",
    "Making \"but\" and \"not\" special tokens resolves this problem.\n",
    "\n",
    "```\n",
    "closest documents to idx=42\n",
    "the effort is sincere and the results are honest , but the film is so bleak that it's hardly watchable . \n",
    " -0.03 0.05 0.23 0.29 0.43 -0.03 -0.76 -0.05 0.93 1000.00 -0.03 0.40 0.23 -0.52 0.84 0.22 0.14 -0.45 1.10\n",
    "label=0\n",
    "\n",
    "GOOD\n",
    "idx=2922 dist=0.1059\n",
    "the cast is uniformly excellent . . . but the film itself is merely mildly charming . \n",
    " -0.03 0.05 0.23 -0.27 0.63 1000.00 -0.03 0.40 -0.49 0.23 -0.03 -1.05 0.82\n",
    "label=0\n",
    "\n",
    "\n",
    "BAD\n",
    "idx=6140 dist=0.1036\n",
    "the film is all a little lit crit 101 , but it's extremely well played and often very funny . \n",
    " -0.06 0.26 0.07 -0.46 -0.38 -0.04 0.00 0.42 1000.00 0.21 -0.50 0.21 -0.46 0.35 0.36 -0.30 1.22\n",
    "label=1\n",
    "```\n",
    "\n",
    "Need to make (.1 - (-.1)) a bigger distance than (.2-.1). We'll try logistic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_newsgroup_expt(categories, ntrees, expt_f):\n",
    "    rand = np.random.RandomState(123456)    \n",
    "    newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                          remove=('headers', 'footers', 'quotes'),\n",
    "                                          categories=categories)\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                         remove=('headers', 'footers', 'quotes'),\n",
    "                                         categories=categories)\n",
    "\n",
    "    vectorizer = CountVectorizer(binary=True, min_df=3)\n",
    "    X_train = vectorizer.fit_transform(newsgroups_train.data).todense()\n",
    "    y_train = newsgroups_train.target\n",
    "    X_test = vectorizer.transform(newsgroups_test.data).todense()\n",
    "    y_test = newsgroups_test.target\n",
    "    print 'train size=', len(y_train), 'test size=', len(y_test)    \n",
    "    expt_f(X_train, y_train, X_test, y_test, vectorizer, ntrees, rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware'], ntrees=100, expt_f=do_expt_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware'], ntrees=100, expt_f=do_expt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['comp.graphics', 'comp.windows.x'], ntrees=100, expt_f=do_expt_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['comp.graphics', 'comp.windows.x'], ntrees=100, expt_f=do_expt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['sci.crypt', 'sci.electronics'], ntrees=100, expt_f=do_expt_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_newsgroup_expt(['sci.crypt', 'sci.electronics'], ntrees=100, expt_f=do_expt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMDB\n",
    "# Sample 1k train and 1k test.\n",
    "def top_feats(clf, x, feature_names):\n",
    "    dot = np.array(x[0])[0] * clf.coef_[0]\n",
    "    return ' '.join(['%s=%g' % (feature_names[i], dot[i]) for i in np.argsort(dot)[::-1][:5]]) + '|||' + \\\n",
    "           ' '.join(['%s=%g' % (feature_names[i], dot[i]) for i in np.argsort(dot)[:5]])\n",
    "\n",
    "def do_imdb_expt(ntrees, expt_f):\n",
    "    rand = np.random.RandomState(123456)    \n",
    "    data = pickle.load(open('/data/aclImdb/data.pkl'))\n",
    "    sample = rand.choice(range(len(data[1].train.data)), size=1000, replace=False)\n",
    "    vectorizer = CountVectorizer(binary=True, min_df=4)\n",
    "    X_train = vectorizer.fit_transform(data[1].train.data[sample]).todense()\n",
    "    y_train = data[1].train.target[sample]\n",
    "    X_test = vectorizer.transform(data[1].test.data[sample]).todense()\n",
    "    y_test = data[1].test.target[sample]\n",
    "    print 'train size=', len(y_train), 'test size=', len(y_test)    \n",
    "    feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "    X_test_new = expt_f(X_train, y_train, X_test, y_test, vectorizer, ntrees, rand)    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_og = clf.predict(X_test)\n",
    "    preds_new = clf.predict(X_test_new)\n",
    "    # old right, new wrong:\n",
    "    print 'OLD RIGHT, NEW WRONG:\\n'\n",
    "    for i, (pred_og, pred_new, truth) in enumerate(zip(preds_og, preds_new, y_test)):\n",
    "        text = data[1].test.data[sample][i]\n",
    "        feats = X_test[i]\n",
    "        if pred_og != pred_new and pred_og == truth:\n",
    "            print '\\n\\npred_og=', pred_og, 'pred_new=', pred_new, 'truth=', truth, \\\n",
    "            'top_feats=\\n', top_feats(clf, feats, feature_names), \\\n",
    "            '\\ntop_feats new=\\n', top_feats(clf, X_test_new[i], feature_names), \\\n",
    "            '\\ntop_feats diff=\\n', top_feats(clf, np.abs(X_test_new[i] - feats), feature_names), \\\n",
    "            '\\ninstance=\\n', data[1].test.data[sample][i]\n",
    "\n",
    "    print '\\nOLD WRONG, NEW RIGHT:\\n'\n",
    "    diff_sums = np.array([0] * len(feature_names))\n",
    "    for i, (pred_og, pred_new, truth) in enumerate(zip(preds_og, preds_new, y_test)):\n",
    "        text = data[1].test.data[sample][i]\n",
    "        feats = X_test[i]\n",
    "        if pred_og != pred_new and pred_new == truth:\n",
    "            print '\\n\\npred=', pred_og, 'pred_new=', pred_new, 'truth=', truth, \\\n",
    "            'top_feats=\\n', top_feats(clf, feats, feature_names), \\\n",
    "            '\\ntop_feats new=\\n', top_feats(clf, X_test_new[i], feature_names), \\\n",
    "            '\\ntop_feats diff=\\n', top_feats(clf, np.abs(X_test_new[i] - feats), feature_names), \\\n",
    "            '\\ninstance=\\n', data[1].test.data[sample][i]    \n",
    "            diff_sums += np.abs(X_test_new[i] - feats).tolist()[0]\n",
    "    print 'most changed features:'\n",
    "    print '\\n'.join(['%s %d' % (feature_names[i], diff_sums[i]) for i in np.argsort(diff_sums)[::-1] if diff_sums[i] > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_imdb_expt(ntrees=100, expt_f=do_expt_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_imdb_expt(ntrees=40, expt_f=do_expt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Without retraining classifier:\n",
    "  - Using top 900 features by chi2, test accuracy goes from 0.809524 to 0.845560\n",
    "  - Somewhat non-monotonic increase in accuracy as number of features increases (e.g., accuracy at 1000 is somewhat less; .839125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twokenize(string, lowercase=True, keep_punctuation=False, collapse_urls=True, collapse_mentions=True, collapse_numbers=True):\n",
    "    if not string:\n",
    "        return []\n",
    "    if lowercase:\n",
    "        string = string.lower()\n",
    "    tokens = []\n",
    "    if collapse_urls:\n",
    "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
    "    if collapse_mentions:\n",
    "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
    "    if collapse_numbers:\n",
    "        string = re.sub('[0-9]+', '99', string)\n",
    "    if keep_punctuation:\n",
    "        tokens = string.split()\n",
    "    else:\n",
    "        tokens = re.sub('\\W+', ' ', string).split()\n",
    "    return tokens\n",
    "\n",
    "def do_ecig_expt(ntrees, expt_f):\n",
    "    rand = np.random.RandomState(123456)  \n",
    "    y = []\n",
    "    tweets = []\n",
    "    for row in csv.DictReader(open('/data/2/uic/elaine/AllSamplesPositiveandNeutral.csv')):\n",
    "        y.append(int(row['sent']))\n",
    "        tweets.append(row['text'])\n",
    "    y = np.array(y)\n",
    "    vectorizer = CountVectorizer(decode_error='ignore', ngram_range=(1, 1), max_df=1.0, min_df=2, tokenizer=twokenize, binary=True)\n",
    "    X = vectorizer.fit_transform(tweets).todense()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print 'read %d instances and %d features; label distribution=%s' % (len(y), len(feature_names), Counter(y))\n",
    "    print 'feature_names=', feature_names[:10]\n",
    "    og_preds = []\n",
    "    og_acc = []\n",
    "    new_preds = []\n",
    "    truths = []\n",
    "    new_acc = []\n",
    "    for train, test in KFold(len(y), n_folds=5, shuffle=True, random_state=rand):\n",
    "        truths.extend(y[test])\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X[train], y[train])\n",
    "        og_pred = clf.predict(X[test])\n",
    "        og_preds.extend(og_pred)\n",
    "        og_acc.append(accuracy_score(y[test], og_pred))\n",
    "\n",
    "        X_test, acc = expt_f(X[train], y[train], X[test], y[test], vectorizer, ntrees, rand)\n",
    "        new_acc.append(acc)\n",
    "    print 'og acc=%g' % np.mean(og_acc)\n",
    "    print 'new acc=%g' % np.mean(new_acc)\n",
    "\n",
    "    #y_train = newsgroups_train.target\n",
    "    #X_test = vectorizer.transform(newsgroups_test.data).todense()\n",
    "    #y_test = newsgroups_test.target\n",
    "    #print 'train size=', len(y_train), 'test size=', len(y_test)    \n",
    "    #expt_f(X_train, y_train, X_test, y_test, vectorizer, ntrees, rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_ecig_expt(20, do_expt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_ecig_expt(20, do_expt_cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "python",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
